import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time

# åˆ†ç±»æ¨¡å— URL
section_urls = [
    "https://www.npr.org/sections/national",
    "https://www.npr.org/sections/world",
    "https://www.npr.org/sections/politics",
    "https://www.npr.org/sections/business",
    "https://www.npr.org/sections/health",
    "https://www.npr.org/sections/science",
    "https://www.npr.org/sections/climate",
    "https://www.npr.org/sections/codeswitch"
]

# ä¿å­˜ç›®å½•
if not os.path.exists('npr_articles'):
    os.makedirs('npr_articles')

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0"
}

seen_titles = set()
articles = []
article_count = 0
max_articles = 100  # æ€»çˆ¬å–æ•°é‡é™åˆ¶


def parse_article(article_url):
    try:
        resp = requests.get(article_url, headers=headers, timeout=10)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, 'html.parser')

        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else 'æ— æ ‡é¢˜'
        if title in seen_titles:
            print(f"æ ‡é¢˜é‡å¤ï¼Œè·³è¿‡é‡å¤æ–‡ç« ï¼š{title}")
            return None
        seen_titles.add(title)

        content_div = soup.find('div', id='storytext')
        if content_div:
            paragraphs = content_div.find_all('p')
            content = '\n'.join(p.get_text(strip=True) for p in paragraphs)
        else:
            content = 'æ— å†…å®¹'

        summary_tag = soup.find('div', class_='teaser')
        summary = summary_tag.get_text(strip=True) if summary_tag else content[:150]

        author_tag = soup.select_one('p.byline__name--block a')
        author = author_tag.get_text(strip=True) if author_tag else None

        time_tag = soup.find('time')
        publish_time = time_tag['datetime'][:10] if time_tag and 'datetime' in time_tag.attrs else None

        word_count = len(content.split())
        crawl_time = datetime.utcnow().isoformat()

        return {
            "url": article_url,
            "title": title,
            "content": content,
            "summary": summary,
            "author": author,
            "publish_time": publish_time,
            "word_count": word_count,
            "crawl_time": crawl_time
        }

    except Exception as e:
        print(f"è§£æå¤±è´¥ï¼š{article_url}, é”™è¯¯ï¼š{e}")
        return None


for section_url in section_urls:
    print(f"è®¿é—®ï¼š{section_url}")
    try:
        resp = requests.get(section_url, headers=headers, timeout=10)
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{section_url}ï¼Œé”™è¯¯ï¼š{e}")
        continue  # è·³åˆ°ä¸‹ä¸€ä¸ªsection_url

    if resp.status_code != 200:
        print(f"é¡µé¢å¤±è´¥ï¼š{section_url}")
        continue

    soup = BeautifulSoup(resp.text, 'html.parser')
    links = soup.select("h2.title a")
    if not links:
        print(f"æ²¡æœ‰æ–‡ç« é“¾æ¥ï¼š{section_url}")
        continue

    for link in links:
        if article_count >= max_articles:
            break
        article_data = parse_article(link['href'])
        if article_data:
            articles.append(article_data)
            article_count += 1

            # ä¿å­˜ä¸º TXT
            txt_path = os.path.join("npr_articles", f"article_{article_count}.txt")
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(article_data['content'])

            print(f"âœ… ä¿å­˜ç¬¬ {article_count} ç¯‡ï¼š{article_data['title']}")
            time.sleep(1)

    if article_count >= max_articles:
        break  # è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°ï¼Œè·³å‡ºå¾ªç¯
    time.sleep(2)


# ä¿å­˜ä¸º JSON æ–‡ä»¶
json_path = os.path.join("npr_articles", "all_articles.json")
with open(json_path, 'w', encoding='utf-8') as f:
    json.dump(articles, f, ensure_ascii=False, indent=4)

print(f"\nğŸ‰ å…±ä¿å­˜ {article_count} ç¯‡æ–‡ç« åˆ° JSONï¼š{json_path}")
