#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœç´¢è¯Šæ–­å·¥å…·ï¼šåˆ†ææŸ¥è¯¢å¤„ç†å’Œç›¸ä¼¼åº¦è®¡ç®—
"""

import sys
import os
sys.path.append('src')

from src.retrieval.search_engine import SearchEngine
import json

class SearchDiagnostics:
    """æœç´¢è¯Šæ–­å·¥å…·ï¼šæ·±å…¥åˆ†ææŸ¥è¯¢å¤„ç†è¿‡ç¨‹"""
    
    def __init__(self, search_engine: SearchEngine):
        self.search_engine = search_engine
    
    def diagnose_query(self, query: str):
        """è¯Šæ–­æŸ¥è¯¢å¤„ç†è¿‡ç¨‹"""
        print(f"ğŸ” è¯Šæ–­æŸ¥è¯¢: '{query}'")
        print("=" * 80)
        
        # 1. æŸ¥è¯¢é¢„å¤„ç†åˆ†æ
        self._analyze_query_processing(query)
        
        # 2. è¯æ±‡åˆ†æ
        self._analyze_vocabulary(query)
        
        # 3. å‘é‡åˆ†æ
        self._analyze_vectors(query)
        
        # 4. ç›¸ä¼¼åº¦åˆ†æ
        self._analyze_similarities(query)
    
    def _analyze_query_processing(self, query: str):
        """åˆ†ææŸ¥è¯¢é¢„å¤„ç†"""
        print("\nğŸ“ 1. æŸ¥è¯¢é¢„å¤„ç†åˆ†æ")
        print("-" * 40)
        
        processor = self.search_engine.query_processor.text_processor
        
        # é€æ­¥å¤„ç†
        print(f"åŸå§‹æŸ¥è¯¢: '{query}'")
        
        cleaned = processor.clean_text(query)
        print(f"æ¸…æ´—å: '{cleaned}'")
        
        tokens = processor.tokenize(cleaned)
        print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        no_stops = processor.remove_stopwords(tokens)
        print(f"å»åœç”¨è¯: {no_stops}")
        
        stemmed = processor.stem_words(no_stops)
        print(f"è¯å¹²æå–: {stemmed}")
        
        final_tokens = processor.process_text(query)
        print(f"æœ€ç»ˆç»“æœ: {final_tokens}")
    
    def _analyze_vocabulary(self, query: str):
        """åˆ†æè¯æ±‡è¡¨ä¿¡æ¯"""
        print("\nğŸ“š 2. è¯æ±‡è¡¨åˆ†æ")
        print("-" * 40)
        
        query_tokens = self.search_engine.query_processor.process_query(query)
        vsm = self.search_engine.query_processor.vector_space_model
        
        print(f"æŸ¥è¯¢è¯æ±‡: {query_tokens}")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(vsm.vocabulary)}")
        
        for token in query_tokens:
            if token in vsm.vocabulary:
                vocab_index = vsm.vocabulary.index(token)
                idf_weight = vsm.idf_weights.get(token, 0)
                
                # ç»Ÿè®¡åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°
                doc_count = 0
                for doc_vector in vsm.document_vectors:
                    if doc_vector[vocab_index] > 0:
                        doc_count += 1
                
                print(f"  è¯æ±‡ '{token}':")
                print(f"    è¯æ±‡è¡¨ç´¢å¼•: {vocab_index}")
                print(f"    IDFæƒé‡: {idf_weight:.3f}")
                print(f"    å‡ºç°åœ¨ {doc_count}/{len(vsm.document_vectors)} ä¸ªæ–‡æ¡£ä¸­")
            else:
                print(f"  è¯æ±‡ '{token}': ä¸åœ¨è¯æ±‡è¡¨ä¸­")
    
    def _analyze_vectors(self, query: str):
        """åˆ†æå‘é‡ä¿¡æ¯"""
        print("\nğŸ”¢ 3. å‘é‡åˆ†æ")
        print("-" * 40)
        
        query_tokens = self.search_engine.query_processor.process_query(query)
        vsm = self.search_engine.query_processor.vector_space_model
        
        # è·å–æŸ¥è¯¢å‘é‡
        query_vector = vsm.get_query_vector(query_tokens)
        
        # åˆ†ææŸ¥è¯¢å‘é‡
        non_zero_count = sum(1 for x in query_vector if x > 0)
        query_norm = sum(x * x for x in query_vector) ** 0.5
        max_weight = max(query_vector) if query_vector else 0
        
        print(f"æŸ¥è¯¢å‘é‡ç»Ÿè®¡:")
        print(f"  å‘é‡ç»´åº¦: {len(query_vector)}")
        print(f"  éé›¶å…ƒç´ : {non_zero_count}")
        print(f"  å‘é‡æ¨¡é•¿: {query_norm:.3f}")
        print(f"  æœ€å¤§æƒé‡: {max_weight:.3f}")
        
        # æ˜¾ç¤ºéé›¶å…ƒç´ 
        print(f"  éé›¶å…ƒç´ è¯¦æƒ…:")
        for i, weight in enumerate(query_vector):
            if weight > 0:
                term = vsm.vocabulary[i]
                print(f"    {term}: {weight:.3f}")
        
        # åˆ†æå‡ ä¸ªæ–‡æ¡£å‘é‡
        print(f"\nå‰3ä¸ªæ–‡æ¡£å‘é‡ç»Ÿè®¡:")
        for doc_id in range(min(3, len(vsm.document_vectors))):
            doc_vector = vsm.document_vectors[doc_id]
            doc_norm = vsm.document_norms[doc_id]
            doc_nonzero = sum(1 for x in doc_vector if x > 0)
            
            print(f"  æ–‡æ¡£{doc_id}: æ¨¡é•¿={doc_norm:.3f}, éé›¶={doc_nonzero}")
    
    def _analyze_similarities(self, query: str):
        """åˆ†æç›¸ä¼¼åº¦è®¡ç®—"""
        print("\nğŸ“Š 4. ç›¸ä¼¼åº¦åˆ†æ")
        print("-" * 40)
        
        # æ‰§è¡Œæœç´¢è·å–ç›¸ä¼¼åº¦
        results = self.search_engine.search(query, top_k=10)
        
        if not results:
            print("æ²¡æœ‰æœç´¢ç»“æœ")
            return
        
        similarities = [r.similarity for r in results]
        
        print(f"ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.3f}")
        print(f"  æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.3f}")
        print(f"  å¹³å‡ç›¸ä¼¼åº¦: {sum(similarities)/len(similarities):.3f}")
        
        # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
        ranges = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0]
        distribution = {f"{ranges[i]:.1f}-{ranges[i+1]:.1f}": 0 for i in range(len(ranges)-1)}
        
        for sim in similarities:
            for i in range(len(ranges)-1):
                if ranges[i] <= sim < ranges[i+1]:
                    range_key = f"{ranges[i]:.1f}-{ranges[i+1]:.1f}"
                    distribution[range_key] += 1
                    break
        
        print(f"  ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
        for range_key, count in distribution.items():
            if count > 0:
                print(f"    {range_key}: {count} ä¸ª")
        
        # è¯¦ç»†åˆ†ææœ€é«˜ç›¸ä¼¼åº¦çš„æ–‡æ¡£
        best_result = results[0]
        print(f"\næœ€ä½³åŒ¹é…æ–‡æ¡£åˆ†æ:")
        print(f"  æ–‡æ¡£ID: {best_result.doc_id}")
        print(f"  ç›¸ä¼¼åº¦: {best_result.similarity:.3f}")
        print(f"  åŒ¹é…è¯æ±‡: {best_result.matched_terms}")
        print(f"  æ ‡é¢˜: {best_result.title[:100]}")
        
        # è®¡ç®—ç‚¹ç§¯åˆ†è§£
        query_tokens = self.search_engine.query_processor.process_query(query)
        vsm = self.search_engine.query_processor.vector_space_model
        query_vector = vsm.get_query_vector(query_tokens)
        doc_vector = vsm.get_document_vector(best_result.doc_id)
        
        # è®¡ç®—ç‚¹ç§¯è´¡çŒ®
        dot_product = sum(q * d for q, d in zip(query_vector, doc_vector))
        query_norm = sum(x * x for x in query_vector) ** 0.5
        doc_norm = vsm.document_norms[best_result.doc_id]
        
        print(f"  ç›¸ä¼¼åº¦è®¡ç®—è¯¦æƒ…:")
        print(f"    ç‚¹ç§¯: {dot_product:.3f}")
        print(f"    æŸ¥è¯¢å‘é‡æ¨¡é•¿: {query_norm:.3f}")
        print(f"    æ–‡æ¡£å‘é‡æ¨¡é•¿: {doc_norm:.3f}")
        print(f"    ä½™å¼¦ç›¸ä¼¼åº¦: {dot_product/(query_norm*doc_norm):.3f}")
    
    def suggest_improvements(self, query: str):
        """å»ºè®®æ”¹è¿›æ–¹æ¡ˆ"""
        print("\nğŸš€ 5. æ”¹è¿›å»ºè®®")
        print("-" * 40)
        
        query_tokens = self.search_engine.query_processor.process_query(query)
        vsm = self.search_engine.query_processor.vector_space_model
        
        suggestions = []
        
        # æ£€æŸ¥è¯æ±‡è¦†ç›–
        missing_tokens = [token for token in query_tokens if token not in vsm.vocabulary]
        if missing_tokens:
            suggestions.append(f"è¯æ±‡è¡¨ç¼ºå¤±: {missing_tokens}")
        
        # æ£€æŸ¥IDFæƒé‡
        low_idf_tokens = []
        for token in query_tokens:
            if token in vsm.idf_weights:
                if vsm.idf_weights[token] < 2.0:  # è¾ƒä½çš„IDF
                    low_idf_tokens.append(token)
        
        if low_idf_tokens:
            suggestions.append(f"é«˜é¢‘è¯æ±‡(ä½IDF): {low_idf_tokens}")
        
        # æ£€æŸ¥æŸ¥è¯¢é•¿åº¦
        if len(query_tokens) < 3:
            suggestions.append("æŸ¥è¯¢è¯æ±‡è¾ƒå°‘ï¼Œå¯è€ƒè™‘æŸ¥è¯¢æ‰©å±•")
        
        # å…·ä½“å»ºè®®
        print("é’ˆå¯¹æ€§æ”¹è¿›å»ºè®®:")
        print("1. ç®—æ³•ä¼˜åŒ–:")
        print("   - ä½¿ç”¨BM25ä»£æ›¿TF-IDF")
        print("   - å¢åŠ æ ‡é¢˜å­—æ®µæƒé‡")
        print("   - å®ç°æŸ¥è¯¢æ‰©å±•")
        
        print("2. é¢„å¤„ç†ä¼˜åŒ–:")
        print("   - è°ƒæ•´è¯å¹²æå–ç­–ç•¥")
        print("   - ä¼˜åŒ–åœç”¨è¯åˆ—è¡¨")
        print("   - ä¿ç•™éƒ¨åˆ†è¯å½¢å˜åŒ–")
        
        print("3. å‘é‡ç©ºé—´ä¼˜åŒ–:")
        print("   - é™ç»´æŠ€æœ¯(å¦‚LSA)")
        print("   - è¯æ±‡è¿‡æ»¤(å»é™¤æé«˜/ä½é¢‘è¯)")
        print("   - å½’ä¸€åŒ–ç­–ç•¥è°ƒæ•´")
        
        if suggestions:
            print("4. å½“å‰æŸ¥è¯¢ç‰¹å®šé—®é¢˜:")
            for suggestion in suggestions:
                print(f"   - {suggestion}")


def main():
    """ä¸»å‡½æ•°"""
    data_file = "data/npr_articles.json"
    
    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return
    
    # åˆå§‹åŒ–æœç´¢å¼•æ“
    print("åˆå§‹åŒ–æœç´¢å¼•æ“...")
    search_engine = SearchEngine(data_file)
    if not search_engine.initialize():
        print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return
    
    # åˆ›å»ºè¯Šæ–­å·¥å…·
    diagnostics = SearchDiagnostics(search_engine)
    
    # è¯Šæ–­ç¤ºä¾‹æŸ¥è¯¢
    test_queries = [
        "climate change",
        "health care",
        "education policy",
        "technology innovation"
    ]
    
    for query in test_queries:
        diagnostics.diagnose_query(query)
        diagnostics.suggest_improvements(query)
        
        response = input(f"\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢ï¼Œè¾“å…¥'q'é€€å‡º: ").strip()
        if response.lower() == 'q':
            break
        print("\n" + "="*100 + "\n")


if __name__ == "__main__":
    main()