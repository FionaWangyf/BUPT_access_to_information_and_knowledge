#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿¡æ¯æŠ½å–ç³»ç»Ÿé›†æˆæµ‹è¯•
æ–‡ä»¶ä½ç½®ï¼štest_extraction_system.py
"""

import sys
import os
import json
import time
sys.path.append('src')

from src.extraction.extraction_manager import ExtractionManager

class MockDocument:
    """æ¨¡æ‹Ÿæ–‡æ¡£ç±»"""
    def __init__(self, doc_id: int, title: str, content: str, summary: str = ""):
        self.doc_id = doc_id
        self.title = title
        self.content = content
        self.summary = summary

def test_extraction_system():
    """æµ‹è¯•å®Œæ•´çš„ä¿¡æ¯æŠ½å–ç³»ç»Ÿ"""
    print("ğŸ¯ ä¿¡æ¯æŠ½å–ç³»ç»Ÿé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–ç®¡ç†å™¨
    print("\nğŸ“‹ æ­¥éª¤1: åˆå§‹åŒ–æŠ½å–ç®¡ç†å™¨")
    config = {
        'enable_regex_extractor': True,
        'regex_confidence_threshold': 0.6,
        'merge_duplicate_entities': True,
        'max_entities_per_type': 50,
        'enable_cache': True
    }
    
    manager = ExtractionManager(config)
    if not manager.initialize():
        print("âŒ ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
        return False
    
    print(f"âœ… ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"ğŸ“‹ æ”¯æŒçš„å®ä½“ç±»å‹: {manager.get_supported_entity_types()}")
    
    # 2. æµ‹è¯•å•æ–‡æœ¬æŠ½å–
    print("\nğŸ“‹ æ­¥éª¤2: æµ‹è¯•å•æ–‡æœ¬æŠ½å–")
    test_text = """
    President Joe Biden announced today that the United States will provide
    $2.5 billion in military aid to Ukraine. The announcement was made at the 
    White House during a meeting with Ukrainian President Volodymyr Zelenskyy.
    
    "This support is crucial for our democratic allies," Biden said during the 
    press conference at 2:30 PM EST. NPR's David Folkenflik reported from 
    Washington, D.C.
    
    The funding will be distributed through the Department of Defense according 
    to Secretary Lloyd Austin. For more information, contact the White House at 
    202-456-1414 or visit https://whitehouse.gov.
    """
    
    results = manager.extract_from_text(test_text, doc_id=1, field="content")
    
    print(f"âœ… å•æ–‡æœ¬æŠ½å–å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªå®ä½“")
    
    # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
    results_by_type = {}
    for result in results:
        if result.entity_type not in results_by_type:
            results_by_type[result.entity_type] = []
        results_by_type[result.entity_type].append(result)
    
    for entity_type, type_results in results_by_type.items():
        print(f"\n  ğŸ·ï¸ {entity_type.upper()} ({len(type_results)} ä¸ª):")
        for result in sorted(type_results, key=lambda x: x.confidence, reverse=True)[:3]:
            print(f"    - {result.entity_value} (ç½®ä¿¡åº¦: {result.confidence:.3f})")
    
    # 3. æµ‹è¯•æ–‡æ¡£å¯¹è±¡æŠ½å–
    print("\nğŸ“‹ æ­¥éª¤3: æµ‹è¯•æ–‡æ¡£å¯¹è±¡æŠ½å–")
    
    test_documents = [
        MockDocument(
            doc_id=1,
            title="Biden Announces Military Aid to Ukraine",
            content="President Biden announced $2.5 billion aid package...",
            summary="US provides military support to Ukraine"
        ),
        MockDocument(
            doc_id=2,
            title="NPR CEO Testifies Before Congress",
            content="Katherine Maher testified about NPR's editorial independence...",
            summary="NPR leadership faces congressional questioning"
        ),
        MockDocument(
            doc_id=3,
            title="Federal Reserve Raises Interest Rates",
            content="The Federal Reserve announced a 0.25% rate increase...",
            summary="Fed continues monetary tightening policy"
        )
    ]
    
    # æµ‹è¯•å•ä¸ªæ–‡æ¡£
    doc_results = manager.extract_from_document(test_documents[0])
    print(f"âœ… å•æ–‡æ¡£æŠ½å–å®Œæˆï¼Œæ‰¾åˆ° {len(doc_results)} ä¸ªå®ä½“")
    
    # 4. æµ‹è¯•æ‰¹é‡å¤„ç†
    print("\nğŸ“‹ æ­¥éª¤4: æµ‹è¯•æ‰¹é‡å¤„ç†")
    
    def progress_callback(current, total, progress):
        if current % 1 == 0:  # æ¯ä¸ªæ–‡æ¡£éƒ½æ˜¾ç¤ºè¿›åº¦
            print(f"    è¿›åº¦: {current}/{total} ({progress:.1%})")
    
    batch_results = manager.extract_from_documents(test_documents, progress_callback)
    
    total_entities = sum(len(results) for results in batch_results.values())
    print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è®¡ {total_entities} ä¸ªå®ä½“")
    
    # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ç»“æœ
    for doc_id, doc_results in batch_results.items():
        print(f"  ğŸ“„ æ–‡æ¡£ {doc_id}: {len(doc_results)} ä¸ªå®ä½“")
    
    # 5. æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“‹ æ­¥éª¤5: æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯")
    stats = manager.get_summary_statistics()
    
    print("ğŸ“Š æŠ½å–ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  ğŸ“„ å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents_processed']}")
    print(f"  ğŸ” æ€»æŠ½å–æ•°: {stats['total_extractions']}")
    print(f"  â­ å¹³å‡ç½®ä¿¡åº¦: {stats['average_confidence']:.3f}")
    print(f"  â±ï¸ æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.3f}ç§’")
    print(f"  ğŸ“Š å¹³å‡æ¯æ–‡æ¡£æŠ½å–æ•°: {stats.get('avg_extractions_per_document', 0):.1f}")
    print(f"  ğŸ¯ æœ‰æŠ½å–ç»“æœçš„æ–‡æ¡£æ¯”ä¾‹: {stats.get('documents_with_extractions_ratio', 0):.1%}")
    
    print(f"\nğŸ“‹ å„ç±»å‹å®ä½“ç»Ÿè®¡:")
    for entity_type, count in stats['extractions_by_type'].items():
        print(f"  {entity_type}: {count} ä¸ª")
    
    print(f"\nğŸ”§ å„æŠ½å–å™¨ç»Ÿè®¡:")
    for extractor, count in stats['extractions_by_extractor'].items():
        print(f"  {extractor}: {count} ä¸ª")
    
    # 6. æµ‹è¯•ç»“æœå¯¼å‡º
    print("\nğŸ“‹ æ­¥éª¤6: æµ‹è¯•ç»“æœå¯¼å‡º")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('results', exist_ok=True)
    
    # å¯¼å‡ºJSONæ ¼å¼
    manager.export_results(batch_results, 'results/extraction_results.json', 'json')
    
    # å¯¼å‡ºCSVæ ¼å¼
    manager.export_results(batch_results, 'results/extraction_results.csv', 'csv')
    
    # å¯¼å‡ºæ–‡æœ¬æ ¼å¼
    manager.export_results(batch_results, 'results/extraction_results.txt', 'txt')
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    manager.save_statistics('results/extraction_statistics.json')
    
    print("âœ… ç»“æœå¯¼å‡ºå®Œæˆ")
    
    # 7. æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print("\nğŸ“‹ æ­¥éª¤7: æµ‹è¯•ç¼“å­˜åŠŸèƒ½")
    
    # ç¬¬ä¸€æ¬¡æŠ½å–
    start_time = time.time()
    results1 = manager.extract_from_text(test_text, doc_id=99, field="cache_test")
    time1 = time.time() - start_time
    
    # ç¬¬äºŒæ¬¡æŠ½å–ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
    start_time = time.time()
    results2 = manager.extract_from_text(test_text, doc_id=99, field="cache_test")
    time2 = time.time() - start_time
    
    print(f"âœ… ç¼“å­˜æµ‹è¯•:")
    print(f"  ç¬¬ä¸€æ¬¡æŠ½å–: {time1:.4f}ç§’, {len(results1)} ä¸ªå®ä½“")
    print(f"  ç¬¬äºŒæ¬¡æŠ½å–: {time2:.4f}ç§’, {len(results2)} ä¸ªå®ä½“")
    print(f"  åŠ é€Ÿæ¯”: {time1/time2 if time2 > 0 else 'N/A':.1f}x")
    
    return True

def test_with_real_npr_data():
    """ä½¿ç”¨çœŸå®NPRæ•°æ®æµ‹è¯•"""
    print("\nğŸŒŸ ä½¿ç”¨çœŸå®NPRæ•°æ®æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åŠ è½½NPRæ•°æ®
        with open('data/npr_articles.json', 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        print(f"âœ… åŠ è½½äº† {len(articles)} ç¯‡NPRæ–‡ç« ")
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        config = {
            'enable_regex_extractor': True,
            'regex_confidence_threshold': 0.7,  # æ›´ä¸¥æ ¼çš„é˜ˆå€¼
            'merge_duplicate_entities': True,
            'max_entities_per_type': 30,
            'enable_cache': True
        }
        
        manager = ExtractionManager(config)
        if not manager.initialize():
            print("âŒ ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        documents = []
        for i, article in enumerate(articles[:10]):  # æµ‹è¯•å‰10ç¯‡
            doc = MockDocument(
                doc_id=i,
                title=article['title'],
                content=article['content'][:2000],  # é™åˆ¶é•¿åº¦ä»¥èŠ‚çœæ—¶é—´
                summary=article.get('summary', '')
            )
            documents.append(doc)
        
        print(f"ğŸ“„ å‡†å¤‡å¤„ç† {len(documents)} ç¯‡æ–‡ç« ")
        
        # æ‰¹é‡å¤„ç†
        def progress_callback(current, total, progress):
            print(f"    å¤„ç†è¿›åº¦: {current}/{total} ({progress:.1%})")
        
        start_time = time.time()
        results = manager.extract_from_documents(documents, progress_callback)
        processing_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        total_entities = sum(len(doc_results) for doc_results in results.values())
        
        print(f"\nâœ… çœŸå®æ•°æ®æµ‹è¯•å®Œæˆ:")
        print(f"  ğŸ“„ å¤„ç†æ–‡ç« : {len(documents)} ç¯‡")
        print(f"  ğŸ” æ€»æŠ½å–æ•°: {total_entities} ä¸ªå®ä½“")
        print(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"  ğŸš€ å¤„ç†é€Ÿåº¦: {len(documents)/processing_time:.1f} æ–‡ç« /ç§’")
        print(f"  ğŸ“Š å¹³å‡æ¯ç¯‡: {total_entities/len(documents):.1f} ä¸ªå®ä½“")
        
        # æ˜¾ç¤ºå„ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for doc_results in results.values():
            for result in doc_results:
                type_counts[result.entity_type] = type_counts.get(result.entity_type, 0) + 1
        
        print(f"\nğŸ“‹ å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {entity_type}: {count} ä¸ª")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ç»“æœ
        print(f"\nğŸ” æŠ½å–ç¤ºä¾‹:")
        for doc_id, doc_results in list(results.items())[:3]:
            print(f"\n  ğŸ“„ æ–‡ç«  {doc_id} ({len(doc_results)} ä¸ªå®ä½“):")
            
            # æŒ‰ç±»å‹åˆ†ç»„
            by_type = {}
            for result in doc_results:
                if result.entity_type not in by_type:
                    by_type[result.entity_type] = []
                by_type[result.entity_type].append(result)
            
            for entity_type, type_results in by_type.items():
                top_results = sorted(type_results, key=lambda x: x.confidence, reverse=True)[:2]
                entities = [f"{r.entity_value}({r.confidence:.2f})" for r in top_results]
                print(f"    {entity_type}: {', '.join(entities)}")
        
        # ä¿å­˜çœŸå®æ•°æ®çš„ç»“æœ
        os.makedirs('results', exist_ok=True)
        manager.export_results(results, 'results/npr_extraction_results.json', 'json')
        manager.save_statistics('results/npr_extraction_statistics.json')
        
        print(f"\nğŸ’¾ çœŸå®æ•°æ®ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
        
        return True
        
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°NPRæ•°æ®æ–‡ä»¶ (data/npr_articles.json)")
        print("   è·³è¿‡çœŸå®æ•°æ®æµ‹è¯•")
        return True
    except Exception as e:
        print(f"âŒ çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ ä¿¡æ¯æŠ½å–ç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    try:
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        if not test_extraction_system():
            print("âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            return False
        
        # çœŸå®æ•°æ®æµ‹è¯•
        if not test_with_real_npr_data():
            print("âŒ çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥")
            return False
        
        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿¡æ¯æŠ½å–ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        print("ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
        print("ğŸ¯ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œå®é™…ä½¿ç”¨")
        
        # ç»™å‡ºä½¿ç”¨å»ºè®®
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("1. å¯ä»¥è°ƒæ•´ regex_confidence_threshold æ¥æ§åˆ¶æŠ½å–ç²¾åº¦")
        print("2. enable_cache=True å¯ä»¥æé«˜é‡å¤å¤„ç†çš„é€Ÿåº¦")
        print("3. max_entities_per_type å¯ä»¥é™åˆ¶æ¯ç§ç±»å‹çš„å®ä½“æ•°é‡")
        print("4. æŸ¥çœ‹ results/ ç›®å½•äº†è§£è¯¦ç»†çš„æŠ½å–ç»“æœ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()