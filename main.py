
"""
NPRæ–‡ç« ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ
é›†æˆBM25ã€å¤šå­—æ®µæƒé‡ã€æ—¶é—´æ–°é²œåº¦ç­‰ä¼˜åŒ–ç®—æ³•
"""

import sys
import os
sys.path.append('src')

from src.retrieval.search_engine import EnhancedSearchEngine
import argparse
import json

def load_config(config_file: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        'use_bm25': True,
        'use_temporal': True,
        'use_multi_field': True,
        'temporal_weight': 0.2,
        'temporal_decay_factor': 0.2,
        'temporal_max_days': 365,
        'title_weight': 3.0,
        'summary_weight': 2.0,
        'content_weight': 1.0,
        'bm25_k1': 1.5,
        'bm25_b': 0.75
    }
    
    if config_file and os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            default_config.update(user_config)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    
    return default_config

def print_welcome():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    print("ğŸš€" + "=" * 78 + "ğŸš€")
    print("ğŸ”" + " " * 20 + "NPRæ–‡ç« ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ" + " " * 20 + "ğŸ”")
    print("ğŸš€" + "=" * 78 + "ğŸš€")
    print("ğŸ¯ é›†æˆå…ˆè¿›æ£€ç´¢ç®—æ³•:")
    print("   â€¢ BM25ç®—æ³• - ä¸šç•Œæ ‡å‡†çš„æ¦‚ç‡æ£€ç´¢æ¨¡å‹")
    print("   â€¢ å¤šå­—æ®µæƒé‡ - æ ‡é¢˜ã€æ‘˜è¦ã€å†…å®¹å·®å¼‚åŒ–è¯„åˆ†")  
    print("   â€¢ æ—¶é—´æ–°é²œåº¦ - æ–°æ–‡ç« ä¼˜å…ˆæ¨è")
    print("   â€¢ æ™ºèƒ½èåˆ - å¤šç»´åº¦ç»¼åˆè¯„åˆ†")
    print("ğŸš€" + "=" * 78 + "ğŸš€")

def main():
    """ä¸»å‡½æ•°"""
    print_welcome()
    
    # è®¾ç½®æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "data/npr_articles.json"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
        print("è¯·ç¡®ä¿æ–‡ç« æ•°æ®æ–‡ä»¶åœ¨dataç›®å½•ä¸‹")
        return
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºå¢å¼ºæœç´¢å¼•æ“
    search_engine = EnhancedSearchEngine(data_file, config)
    
    # åˆå§‹åŒ–æœç´¢å¼•æ“
    print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–å¢å¼ºæœç´¢å¼•æ“ï¼Œè¯·ç¨å€™...")
    if not search_engine.initialize():
        print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return
    
    print("\nâœ… å¢å¼ºæœç´¢å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼")
    
    # å¯åŠ¨äº¤äº’å¼æœç´¢
    search_engine.interactive_search()

def demo_search():
    """æ¼”ç¤ºæœç´¢åŠŸèƒ½"""
    print_welcome()
    print("\nğŸ¯ æ¼”ç¤ºæ¨¡å¼ï¼šé¢„è®¾æŸ¥è¯¢æµ‹è¯•")
    print("=" * 50)
    
    data_file = "data/npr_articles.json"
    
    if not os.path.exists(data_file):
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
        return
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºå’Œåˆå§‹åŒ–æœç´¢å¼•æ“
    search_engine = EnhancedSearchEngine(data_file, config)
    
    if not search_engine.initialize():
        print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return
    
    # é¢„è®¾çš„æ¼”ç¤ºæŸ¥è¯¢
    demo_queries = [
        {
            "query": "climate change global warming",
            "description": "æ°”å€™å˜åŒ–å’Œå…¨çƒå˜æš–ç›¸å…³æ–°é—»"
        },
        {
            "query": "health care medical treatment", 
            "description": "åŒ»ç–—ä¿å¥å’Œæ²»ç–—ç›¸å…³æŠ¥é“"
        },
        {
            "query": "education school students",
            "description": "æ•™è‚²å’Œå­¦æ ¡ç›¸å…³æ–‡ç« "
        },
        {
            "query": "politics government election",
            "description": "æ”¿æ²»å’Œæ”¿åºœé€‰ä¸¾æ–°é—»"
        },
        {
            "query": "technology artificial intelligence AI",
            "description": "ç§‘æŠ€å’Œäººå·¥æ™ºèƒ½ç›¸å…³å†…å®¹"
        },
        {
            "query": "economy financial market business",
            "description": "ç»æµé‡‘èå’Œå•†ä¸šæŠ¥é“"
        }
    ]
    
    print("\nğŸ” å¼€å§‹æ¼”ç¤ºæœç´¢...")
    
    for i, demo in enumerate(demo_queries):
        query = demo["query"]
        description = demo["description"]
        
        print(f"\n{'='*90}")
        print(f"ğŸ¯ æ¼”ç¤ºæŸ¥è¯¢ {i+1}: '{query}'")
        print(f"ğŸ“ æè¿°: {description}")
        print(f"{'='*90}")
        
        # æ¯”è¾ƒä¸åŒç®—æ³•
        print(f"\nğŸ”¬ ç®—æ³•æ€§èƒ½æ¯”è¾ƒ:")
        comparison_results = search_engine.compare_algorithms(query, top_k=3)
        search_engine.display_comparison_results(comparison_results)
        
        # æ˜¾ç¤ºå¢å¼ºç®—æ³•çš„è¯¦ç»†ç»“æœ
        print(f"\nğŸ¯ å¢å¼ºç®—æ³•è¯¦ç»†ç»“æœ:")
        enhanced_results = comparison_results.get('enhanced', [])
        search_engine.display_results(enhanced_results, show_snippet=True, show_scores=True)
        
        # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
        if enhanced_results:
            stats = search_engine.query_processor.get_search_stats(enhanced_results)
            print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.3f}")
                else:
                    print(f"  {key}: {value}")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­
        if i < len(demo_queries) - 1:
            response = input(f"\nâ­ï¸ æŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤ºæŸ¥è¯¢ï¼Œè¾“å…¥'q'é€€å‡º: ").strip()
            if response.lower() == 'q':
                break
    
    print("\nğŸ¯ æ¼”ç¤ºå®Œæˆï¼")

def benchmark_algorithms():
    """ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print_welcome()
    print("\nğŸ ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    data_file = "data/npr_articles.json"
    
    if not os.path.exists(data_file):
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
        return
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºæœç´¢å¼•æ“
    search_engine = EnhancedSearchEngine(data_file, config)
    
    if not search_engine.initialize():
        print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return
    
    # åŸºå‡†æµ‹è¯•æŸ¥è¯¢
    benchmark_queries = [
        "climate change",
        "health care",
        "education policy",
        "economic growth",
        "technology innovation",
        "political election",
        "scientific research",
        "social justice",
        "environmental protection",
        "public health"
    ]
    
    algorithms = ["tfidf", "bm25", "enhanced"]
    algorithm_stats = {alg: {"total_time": 0, "avg_scores": []} for alg in algorithms}
    
    print(f"\nğŸ”¬ å¼€å§‹åŸºå‡†æµ‹è¯• ({len(benchmark_queries)} ä¸ªæŸ¥è¯¢)")
    
    for i, query in enumerate(benchmark_queries):
        print(f"\næµ‹è¯•æŸ¥è¯¢ {i+1}/{len(benchmark_queries)}: '{query}'")
        
        for algorithm in algorithms:
            import time
            start_time = time.time()
            
            results = search_engine.search(query, top_k=10, algorithm=algorithm)
            
            end_time = time.time()
            search_time = end_time - start_time
            
            algorithm_stats[algorithm]["total_time"] += search_time
            
            if results:
                avg_score = sum(r.similarity for r in results) / len(results)
                algorithm_stats[algorithm]["avg_scores"].append(avg_score)
            
            print(f"  {algorithm:>8}: {search_time:.3f}s, {len(results)} ç»“æœ")
    
    # æ˜¾ç¤ºåŸºå‡†æµ‹è¯•ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ† åŸºå‡†æµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    
    for algorithm, stats in algorithm_stats.items():
        avg_time = stats["total_time"] / len(benchmark_queries)
        avg_score = sum(stats["avg_scores"]) / len(stats["avg_scores"]) if stats["avg_scores"] else 0
        
        print(f"\nğŸ“Š {algorithm.upper()} ç®—æ³•:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f} ç§’")
        print(f"  å¹³å‡ç›¸å…³åº¦åˆ†æ•°: {avg_score:.3f}")
        print(f"  æ€»è€—æ—¶: {stats['total_time']:.3f} ç§’")

def create_sample_config():
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    config = {
        "use_bm25": True,
        "use_temporal": True,
        "use_multi_field": True,
        "temporal_weight": 0.2,
        "temporal_decay_factor": 0.2,
        "temporal_max_days": 365,
        "title_weight": 3.0,
        "summary_weight": 2.0,
        "content_weight": 1.0,
        "bm25_k1": 1.5,
        "bm25_b": 0.75
    }
    
    config_file = "config_sample.json"
    
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        print("ğŸ“ ä½ å¯ä»¥ä¿®æ”¹æ­¤æ–‡ä»¶æ¥è‡ªå®šä¹‰æœç´¢å¼•æ“å‚æ•°")
    except Exception as e:
        print(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="NPRæ–‡ç« ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ - å¢å¼ºç‰ˆ", 
                                   formatter_class=argparse.RawDescriptionHelpFormatter,
                                   epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py                           # å¯åŠ¨äº¤äº’æ¨¡å¼
  python main.py --demo                    # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
  python main.py --benchmark               # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
  python main.py --query "climate change" # å•æ¬¡æŸ¥è¯¢
  python main.py --config config.json     # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  python main.py --create-config          # åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
                                   """)
    
    parser.add_argument("--demo", action="store_true", help="è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
    parser.add_argument("--benchmark", action="store_true", help="è¿è¡Œç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--query", type=str, help="ç›´æ¥æ‰§è¡Œå•æ¬¡æŸ¥è¯¢")
    parser.add_argument("--algorithm", type=str, choices=["tfidf", "bm25", "enhanced"], 
                       default="enhanced", help="æŒ‡å®šæœç´¢ç®—æ³•")
    parser.add_argument("--config", type=str, help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--top-k", type=int, default=10, help="è¿”å›ç»“æœæ•°é‡")
    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    
    args = parser.parse_args()
    
    try:
        if args.create_config:
            create_sample_config()
            
        elif args.benchmark:
            benchmark_algorithms()
            
        elif args.demo:
            demo_search()
            
        elif args.query:
            # å•æ¬¡æŸ¥è¯¢æ¨¡å¼
            data_file = "data/npr_articles.json"
            if not os.path.exists(data_file):
                print(f"âŒ é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
                sys.exit(1)
            
            config = load_config(args.config)
            search_engine = EnhancedSearchEngine(data_file, config)
            
            if search_engine.initialize():
                print(f"\nğŸ” æ‰§è¡ŒæŸ¥è¯¢: '{args.query}' (ç®—æ³•: {args.algorithm})")
                results = search_engine.search(args.query, top_k=args.top_k, algorithm=args.algorithm)
                search_engine.display_results(results, show_snippet=True, show_scores=True)
                
                # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
                if results:
                    stats = search_engine.query_processor.get_search_stats(results)
                    print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
                    for key, value in stats.items():
                        if isinstance(value, float):
                            print(f"  {key}: {value:.3f}")
                        else:
                            print(f"  {key}: {value}")
            else:
                print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        else:
            # é»˜è®¤äº¤äº’æ¨¡å¼
            main()
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()