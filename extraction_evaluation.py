#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿¡æ¯æŠ½å–ç³»ç»Ÿäººå·¥è¯„ä»·å·¥å…·
æ–‡ä»¶ä½ç½®ï¼šextraction_evaluation.py
"""

import sys
import os
import json
import time
import argparse
from typing import List, Dict, Any, Optional

sys.path.append('src')

from src.extraction.extraction_manager import ExtractionManager
from src.evaluation.extraction_evaluator import ExtractionEvaluator
from src.evaluation.extraction_metrics import ExtractionMetrics


class ExtractionEvaluationSystem:
    """ä¿¡æ¯æŠ½å–ç³»ç»Ÿè¯„ä»·å·¥å…·"""

    def __init__(self, data_file: str = "data/npr_articles.json"):
        self.data_file = data_file
        self.extraction_manager = None
        self.evaluator = None
        self.metrics_calculator = ExtractionMetrics()

        # è¯„ä»·é…ç½®
        self.evaluation_config = {
            'sample_size': 10,  # ç”¨äºè¯„ä»·çš„æ–‡æ¡£æ ·æœ¬æ•°
            'entities_per_doc': 10,  # æ¯ç¯‡æ–‡æ¡£è¯„ä»·çš„å®ä½“æ•°ä¸Šé™
            'evaluation_file': 'results/extraction_evaluation.json',
            'sample_file': 'results/evaluation_sample.json'
        }

    def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯„ä»·ç³»ç»Ÿ"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–ä¿¡æ¯æŠ½å–è¯„ä»·ç³»ç»Ÿ...")

            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            if not os.path.exists(self.data_file):
                print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
                return False

            # åˆå§‹åŒ–æŠ½å–ç®¡ç†å™¨
            config = {
                'enable_regex_extractor': True,
                'regex_confidence_threshold': 0.6,
                'merge_duplicate_entities': True,
                'max_entities_per_type': 50,
                'enable_cache': True,
            }

            self.extraction_manager = ExtractionManager(config)
            if not self.extraction_manager.initialize():
                print("âŒ æŠ½å–ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
                return False

            # åˆå§‹åŒ–è¯„ä»·å™¨
            self.evaluator = ExtractionEvaluator(
                self.extraction_manager,
                self.evaluation_config
            )

            # åˆ›å»ºç»“æœç›®å½•
            os.makedirs('results', exist_ok=True)

            print("âœ… è¯„ä»·ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
            return True

        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def create_evaluation_sample(self, sample_size: int = None):
        """åˆ›å»ºè¯„ä»·æ ·æœ¬"""
        if sample_size:
            self.evaluation_config['sample_size'] = sample_size

        print(f"ğŸ“Š åˆ›å»ºè¯„ä»·æ ·æœ¬ (æ ·æœ¬å¤§å°: {self.evaluation_config['sample_size']})")

        try:
            # åŠ è½½æ–‡æ¡£æ•°æ®
            with open(self.data_file, 'r', encoding='utf-8') as f:
                articles = json.load(f)

            # åˆ›å»ºæ ·æœ¬
            sample_docs = self.evaluator.create_sample(articles)

            print(f"âœ… å·²åˆ›å»ºåŒ…å« {len(sample_docs)} ç¯‡æ–‡æ¡£çš„è¯„ä»·æ ·æœ¬")

            # å¯¹æ ·æœ¬è¿›è¡ŒæŠ½å–
            print("ğŸ” æ­£åœ¨å¯¹æ ·æœ¬æ–‡æ¡£è¿›è¡Œä¿¡æ¯æŠ½å–...")
            sample_results = self.evaluator.extract_from_sample(sample_docs)

            print(f"âœ… æ ·æœ¬æŠ½å–å®Œæˆï¼Œå…±æŠ½å– {sum(len(results) for results in sample_results.values())} ä¸ªå®ä½“")

            return sample_results

        except Exception as e:
            print(f"âŒ åˆ›å»ºè¯„ä»·æ ·æœ¬å¤±è´¥: {e}")
            return None

    def run_manual_evaluation(self, sample_results: Dict = None):
        """è¿è¡Œäººå·¥è¯„ä»·"""
        print("\n" + "=" * 60)
        print("ğŸ¯ ä¿¡æ¯æŠ½å–ç³»ç»Ÿäººå·¥è¯„ä»·")
        print("=" * 60)

        if sample_results is None:
            # å°è¯•åŠ è½½å·²æœ‰çš„æ ·æœ¬ç»“æœ
            try:
                sample_results = self.evaluator.load_sample_results()
                if not sample_results:
                    print("ğŸ“Š æœªæ‰¾åˆ°å·²æœ‰æ ·æœ¬ï¼Œæ­£åœ¨åˆ›å»ºæ–°æ ·æœ¬...")
                    sample_results = self.create_evaluation_sample()
                    if not sample_results:
                        return
            except Exception as e:
                print(f"âŒ åŠ è½½æ ·æœ¬å¤±è´¥: {e}")
                return

        print("\nğŸ“– è¯„ä»·è¯´æ˜:")
        print("  å¯¹äºæ¯ä¸ªæŠ½å–çš„å®ä½“ï¼Œæ‚¨éœ€è¦è¯„åˆ¤å…¶æ˜¯å¦æ­£ç¡®ï¼š")
        print("  âœ“ (y) - æ­£ç¡®ï¼šå®ä½“ç±»å‹å’Œå€¼éƒ½æ­£ç¡®")
        print("  âœ— (n) - é”™è¯¯ï¼šå®ä½“ç±»å‹æˆ–å€¼ä¸æ­£ç¡®")
        print("  ? (s) - è·³è¿‡ï¼šä¸ç¡®å®šæˆ–æš‚æ—¶è·³è¿‡")
        print("  ğŸ“ åœ¨æ¯ç¯‡æ–‡æ¡£è¯„ä»·å®Œæˆåï¼Œæ‚¨è¿˜å¯ä»¥æ·»åŠ é—æ¼çš„å®ä½“")

        # å¼€å§‹äººå·¥è¯„ä»·
        self.evaluator.start_manual_evaluation(sample_results)

    def calculate_metrics(self):
        """è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
        print("\nğŸ“Š è®¡ç®—è¯„ä»·æŒ‡æ ‡...")

        eval_file = self.evaluation_config['evaluation_file']
        if not os.path.exists(eval_file):
            print(f"âŒ è¯„ä»·æ–‡ä»¶ä¸å­˜åœ¨: {eval_file}")
            print("è¯·å…ˆè¿›è¡Œäººå·¥è¯„ä»·")
            return

        try:
            # åŠ è½½è¯„ä»·æ•°æ®
            evaluation_data = self.evaluator.load_evaluation_results()
            if not evaluation_data:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä»·æ•°æ®")
                return

            # è®¡ç®—æŒ‡æ ‡
            metrics = self.metrics_calculator.calculate_overall_metrics(evaluation_data)

            # æ˜¾ç¤ºç»“æœ
            self._display_metrics(metrics)

            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            self._save_evaluation_report(metrics, evaluation_data)

        except Exception as e:
            print(f"âŒ è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def _display_metrics(self, metrics: Dict[str, Any]):
        """æ˜¾ç¤ºè¯„ä»·æŒ‡æ ‡"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ ä¿¡æ¯æŠ½å–ç³»ç»Ÿè¯„ä»·ç»“æœ")
        print("=" * 60)

        # æ€»ä½“æŒ‡æ ‡
        overall = metrics.get('overall', {})
        print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½:")
        print(f"  ç²¾ç¡®ç‡ (Precision): {overall.get('precision', 0):.3f}")
        print(f"  å¬å›ç‡ (Recall):    {overall.get('recall', 0):.3f}")
        print(f"  F1åˆ†æ•° (F1-Score):  {overall.get('f1_score', 0):.3f}")
        print(f"  æ€»æŠ½å–å®ä½“æ•°:       {overall.get('total_extracted', 0)}")
        print(f"  æ­£ç¡®å®ä½“æ•°:         {overall.get('correct_entities', 0)}")
        print(f"  é—æ¼å®ä½“æ•°:         {overall.get('missed_entities', 0)}")

        # å„ç±»å‹æŒ‡æ ‡
        by_type = metrics.get('by_type', {})
        if by_type:
            print(f"\nğŸ“‹ å„å®ä½“ç±»å‹æ€§èƒ½:")
            print(f"{'ç±»å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'æŠ½å–æ•°':<6} {'æ­£ç¡®æ•°':<6}")
            print(f"{'-' * 60}")

            for entity_type, type_metrics in by_type.items():
                precision = type_metrics.get('precision', 0)
                recall = type_metrics.get('recall', 0)
                f1 = type_metrics.get('f1_score', 0)
                extracted = type_metrics.get('extracted_count', 0)
                correct = type_metrics.get('correct_count', 0)

                print(f"{entity_type:<12} {precision:<8.3f} {recall:<8.3f} {f1:<8.3f} {extracted:<6} {correct:<6}")

        # ç½®ä¿¡åº¦åˆ†æ
        confidence_analysis = metrics.get('confidence_analysis', {})
        if confidence_analysis:
            print(f"\nğŸšï¸ ç½®ä¿¡åº¦åˆ†æ:")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence_analysis.get('average_confidence', 0):.3f}")
            print(f"  é«˜ç½®ä¿¡åº¦å®ä½“ (>0.8): {confidence_analysis.get('high_confidence_count', 0)}")
            print(f"  ä¸­ç½®ä¿¡åº¦å®ä½“ (0.5-0.8): {confidence_analysis.get('medium_confidence_count', 0)}")
            print(f"  ä½ç½®ä¿¡åº¦å®ä½“ (<0.5): {confidence_analysis.get('low_confidence_count', 0)}")

    def _save_evaluation_report(self, metrics: Dict[str, Any], evaluation_data: Dict[str, Any]):
        """ä¿å­˜è¯„ä»·æŠ¥å‘Š"""
        report = {
            'evaluation_summary': {
                'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_documents_evaluated': len(evaluation_data),
                'total_entities_evaluated': sum(
                    len(doc_data.get('evaluated_entities', []))
                    for doc_data in evaluation_data.values()
                )
            },
            'metrics': metrics,
            'detailed_results': evaluation_data
        }

        report_file = 'results/extraction_evaluation_report.json'
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“‹ è¯¦ç»†è¯„ä»·æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    def load_external_extraction_results(self, file_path: str) -> Optional[Dict]:
        """ä»å·²æœ‰ JSON æ–‡ä»¶åŠ è½½æŠ½å–ç»“æœï¼ˆç”¨äºäººå·¥è¯„ä»·ï¼‰"""
        if not os.path.exists(file_path):
            print(f"âŒ æŒ‡å®šçš„æŠ½å–ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“‚ æˆåŠŸåŠ è½½å¤–éƒ¨æŠ½å–ç»“æœï¼Œå…± {len(data)} ç¯‡æ–‡æ¡£")
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½æŠ½å–ç»“æœå¤±è´¥: {e}")
            return None

    def interactive_menu(self):
        """äº¤äº’å¼èœå•"""
        while True:
            print(f"\n{'=' * 50}")
            print(f"ğŸ”¬ ä¿¡æ¯æŠ½å–ç³»ç»Ÿè¯„ä»·å·¥å…·")
            print(f"{'=' * 50}")
            print(f"1. åˆ›å»ºè¯„ä»·æ ·æœ¬")
            print(f"2. è¿è¡Œäººå·¥è¯„ä»·")
            print(f"3. è®¡ç®—è¯„ä»·æŒ‡æ ‡")
            print(f"4. æŸ¥çœ‹è¯„ä»·çŠ¶æ€")
            print(f"5. å¯¼å‡ºè¯„ä»·æ•°æ®")
            print(f"6. ç³»ç»Ÿè®¾ç½®")
            print(f"7. é€€å‡º")

            try:
                choice = input(f"\nè¯·é€‰æ‹©åŠŸèƒ½ (1-7): ").strip()

                if choice == '1':
                    sample_size = input("è¯·è¾“å…¥æ ·æœ¬å¤§å° (é»˜è®¤50): ").strip()
                    if sample_size.isdigit():
                        sample_size = int(sample_size)
                    else:
                        sample_size = 50
                    self.create_evaluation_sample(sample_size)

                elif choice == '2':
                #     self.run_manual_evaluation()
                    file_path = input("è¯·è¾“å…¥æŠ½å–ç»“æœ JSON æ–‡ä»¶è·¯å¾„: ").strip()
                    sample_results = self.load_external_extraction_results(file_path)
                    if sample_results:
                        self.run_manual_evaluation(sample_results)

                elif choice == '3':
                    self.calculate_metrics()

                elif choice == '4':
                    self._show_evaluation_status()

                elif choice == '5':
                    self._export_evaluation_data()

                elif choice == '6':
                    self._show_settings()

                elif choice == '7':
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-7")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

    def _show_evaluation_status(self):
        """æ˜¾ç¤ºè¯„ä»·çŠ¶æ€"""
        print("\nğŸ“Š è¯„ä»·çŠ¶æ€:")

        # æ£€æŸ¥æ ·æœ¬æ–‡ä»¶
        sample_file = self.evaluation_config['sample_file']
        if os.path.exists(sample_file):
            try:
                with open(sample_file, 'r', encoding='utf-8') as f:
                    sample_data = json.load(f)
                print(f"  ğŸ“„ è¯„ä»·æ ·æœ¬: {len(sample_data)} ç¯‡æ–‡æ¡£")
            except:
                print(f"  ğŸ“„ è¯„ä»·æ ·æœ¬: æ–‡ä»¶å­˜åœ¨ä½†æ— æ³•è¯»å–")
        else:
            print(f"  ğŸ“„ è¯„ä»·æ ·æœ¬: æœªåˆ›å»º")

        # æ£€æŸ¥è¯„ä»·æ–‡ä»¶
        eval_file = self.evaluation_config['evaluation_file']
        if os.path.exists(eval_file):
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)
                evaluated_docs = len(eval_data)
                total_evaluations = sum(
                    len(doc_data.get('evaluated_entities', []))
                    for doc_data in eval_data.values()
                )
                print(f"  âœ… å·²è¯„ä»·æ–‡æ¡£: {evaluated_docs} ç¯‡")
                print(f"  âœ… å·²è¯„ä»·å®ä½“: {total_evaluations} ä¸ª")
            except:
                print(f"  âœ… è¯„ä»·è¿›åº¦: æ–‡ä»¶å­˜åœ¨ä½†æ— æ³•è¯»å–")
        else:
            print(f"  âœ… è¯„ä»·è¿›åº¦: æœªå¼€å§‹")

    def _export_evaluation_data(self):
        """å¯¼å‡ºè¯„ä»·æ•°æ®"""
        print("\nğŸ“¤ å¯¼å‡ºè¯„ä»·æ•°æ®...")
        formats = ['json', 'csv', 'xlsx']

        print("æ”¯æŒçš„æ ¼å¼:")
        for i, fmt in enumerate(formats, 1):
            print(f"  {i}. {fmt.upper()}")

        try:
            choice = input("è¯·é€‰æ‹©æ ¼å¼ (1-3): ").strip()
            if choice in ['1', '2', '3']:
                fmt = formats[int(choice) - 1]
                output_file = f"results/extraction_evaluation_export.{fmt}"

                # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„å¯¼å‡ºé€»è¾‘
                print(f"âœ… è¯„ä»·æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
        except:
            print("âŒ å¯¼å‡ºå¤±è´¥")

    def _show_settings(self):
        """æ˜¾ç¤ºå’Œä¿®æ”¹è®¾ç½®"""
        print("\nâš™ï¸ å½“å‰è®¾ç½®:")
        for key, value in self.evaluation_config.items():
            print(f"  {key}: {value}")

        print("\nè¦ä¿®æ”¹è®¾ç½®ï¼Œè¯·ç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶æˆ–é‡æ–°è¿è¡Œç¨‹åº")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¿¡æ¯æŠ½å–ç³»ç»Ÿäººå·¥è¯„ä»·å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python extraction_evaluation.py                    # äº¤äº’å¼æ¨¡å¼
  python extraction_evaluation.py --sample 30       # åˆ›å»º30ä¸ªæ ·æœ¬
  python extraction_evaluation.py --evaluate         # ç›´æ¥å¼€å§‹è¯„ä»·
  python extraction_evaluation.py --metrics          # è®¡ç®—æŒ‡æ ‡
        """
    )

    parser.add_argument('--sample', type=int, help='åˆ›å»ºè¯„ä»·æ ·æœ¬ï¼ˆæŒ‡å®šæ ·æœ¬å¤§å°ï¼‰')
    parser.add_argument('--evaluate', action='store_true', help='è¿è¡Œäººå·¥è¯„ä»·')
    parser.add_argument('--metrics', action='store_true', help='è®¡ç®—è¯„ä»·æŒ‡æ ‡')
    parser.add_argument('--data-file', default='data/npr_articles.json', help='æ•°æ®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä»·ç³»ç»Ÿ
    eval_system = ExtractionEvaluationSystem(args.data_file)

    if not eval_system.initialize():
        print("âŒ è¯„ä»·ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return 1

    try:
        if args.sample:
            eval_system.create_evaluation_sample(args.sample)
        elif args.evaluate:
            eval_system.run_manual_evaluation()
        elif args.metrics:
            eval_system.calculate_metrics()
        else:
            # é»˜è®¤äº¤äº’æ¨¡å¼
            eval_system.interactive_menu()

        return 0

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
        return 0
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())