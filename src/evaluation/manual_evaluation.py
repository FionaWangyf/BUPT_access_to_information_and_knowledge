import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from typing import List, Dict, Any, Tuple
from retrieval.search_engine import EnhancedSearchEngine
from evaluation.test_queries import TestQuery, TestQueryManager
import json
import time

class ManualEvaluator:
    """äººå·¥è¯„ä»·å·¥å…·ï¼šå¸®åŠ©ç”¨æˆ·å¯¹æœç´¢ç»“æœè¿›è¡Œäººå·¥æ ‡æ³¨å’Œè¯„ä»·"""
    
    def __init__(self, search_engine: EnhancedSearchEngine, query_manager: TestQueryManager):
        self.search_engine = search_engine
        self.query_manager = query_manager
        self.evaluation_file = "results/manual_evaluation.json"
        self.current_evaluations = {}  # {query_id: evaluation_data}
    
    def load_or_create_queries(self):
        """åŠ è½½æˆ–åˆ›å»ºæµ‹è¯•æŸ¥è¯¢"""
        if not self.query_manager.load_queries():
            print("åˆ›å»ºé»˜è®¤æµ‹è¯•æŸ¥è¯¢é›†åˆ...")
            self.query_manager.create_default_queries()
            self.query_manager.save_queries()
    
    def evaluate_query(self, query_id: int, max_results: int = 10) -> bool:
        """å¯¹å•ä¸ªæŸ¥è¯¢è¿›è¡Œäººå·¥è¯„ä»·"""
        query = self.query_manager.get_query(query_id)
        if not query:
            print(f"æŸ¥è¯¢ ID {query_id} ä¸å­˜åœ¨")
            return False
        
        print(f"\n{'='*80}")
        print(f"äººå·¥è¯„ä»· - æŸ¥è¯¢ {query_id}")
        print(f"{'='*80}")
        print(f"æŸ¥è¯¢æ–‡æœ¬: {query.query_text}")
        print(f"æŸ¥è¯¢æè¿°: {query.description}")
        
        # æ‰§è¡Œæœç´¢
        print(f"\næ­£åœ¨æœç´¢...")
        results = self.search_engine.search(query.query_text, top_k=max_results)
        
        if not results:
            print("æ²¡æœ‰æœç´¢ç»“æœ")
            return False
        
        # ä¿å­˜ç³»ç»Ÿç»“æœ
        doc_ids = [r.doc_id for r in results]
        query.set_system_results(doc_ids)
        
        print(f"\næ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼Œè¯·é€ä¸€è¯„ä»·ç›¸å…³æ€§:")
        print("è¯„åˆ†æ ‡å‡†:")
        print("  3 - éå¸¸ç›¸å…³ (å®Œå…¨åŒ¹é…æŸ¥è¯¢æ„å›¾)")
        print("  2 - ç›¸å…³ (ä¸æŸ¥è¯¢æœ‰æ˜ç¡®å…³è”)")
        print("  1 - éƒ¨åˆ†ç›¸å…³ (æœ‰ä¸€å®šå…³è”ä½†ä¸å¤ªç¬¦åˆ)")
        print("  0 - ä¸ç›¸å…³ (ä¸æŸ¥è¯¢æ— å…³)")
        print("  s - è·³è¿‡å½“å‰ç»“æœ")
        print("  q - é€€å‡ºå½“å‰æŸ¥è¯¢è¯„ä»·")
        
        evaluation_data = {
            'query_id': query_id,
            'query_text': query.query_text,
            'timestamp': time.time(),
            'results_evaluated': [],
            'relevance_judgments': {}
        }
        
        for i, result in enumerate(results):
            print(f"\n{'-'*60}")
            print(f"ç»“æœ {i+1}/{len(results)}")
            print(f"æ–‡æ¡£ID: {result.doc_id}")
            print(f"ç³»ç»Ÿç›¸ä¼¼åº¦: {result.similarity:.3f}")
            print(f"æ ‡é¢˜: {result.title}")
            print(f"URL: {result.url}")
            print(f"åŒ¹é…è¯æ±‡: {', '.join(result.matched_terms)}")
            print(f"å†…å®¹æ‘˜è¦: {result.snippet}")
            
            while True:
                try:
                    score_input = input(f"\nè¯·è¯„ä»·ç›¸å…³æ€§ (0-3, s=è·³è¿‡, q=é€€å‡º): ").strip().lower()
                    
                    if score_input == 'q':
                        print("é€€å‡ºå½“å‰æŸ¥è¯¢è¯„ä»·")
                        return True
                    elif score_input == 's':
                        print("è·³è¿‡å½“å‰ç»“æœ")
                        break
                    elif score_input in ['0', '1', '2', '3']:
                        score = int(score_input)
                        
                        # è®°å½•è¯„ä»·
                        evaluation_data['results_evaluated'].append({
                            'doc_id': result.doc_id,
                            'rank': i + 1,
                            'similarity': result.similarity,
                            'relevance_score': score,
                            'title': result.title
                        })
                        evaluation_data['relevance_judgments'][str(result.doc_id)] = score
                        
                        # æ›´æ–°æŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£
                        if score > 0:
                            query.add_relevant_document(result.doc_id, score)
                        
                        break
                    else:
                        print("è¯·è¾“å…¥æœ‰æ•ˆçš„è¯„åˆ† (0-3, s, q)")
                        
                except KeyboardInterrupt:
                    print("\n\nè¯„ä»·è¢«ä¸­æ–­")
                    return True
                except Exception as e:
                    print(f"è¾“å…¥é”™è¯¯: {e}")
        
        # ä¿å­˜è¯„ä»·ç»“æœ
        self.current_evaluations[str(query_id)] = evaluation_data
        self._save_evaluation()
        
        # æ˜¾ç¤ºè¯„ä»·æ€»ç»“
        self._show_evaluation_summary(evaluation_data)
        
        return True
    
    def _show_evaluation_summary(self, evaluation_data: Dict[str, Any]):
        """æ˜¾ç¤ºè¯„ä»·æ€»ç»“"""
        results = evaluation_data['results_evaluated']
        if not results:
            return
        
        print(f"\n{'='*60}")
        print(f"è¯„ä»·æ€»ç»“")
        print(f"{'='*60}")
        
        relevance_counts = {0: 0, 1: 0, 2: 0, 3: 0}
        for result in results:
            score = result['relevance_score']
            relevance_counts[score] += 1
        
        print(f"å·²è¯„ä»·ç»“æœæ•°: {len(results)}")
        print(f"ç›¸å…³æ€§åˆ†å¸ƒ:")
        print(f"  éå¸¸ç›¸å…³ (3): {relevance_counts[3]} ä¸ª")
        print(f"  ç›¸å…³ (2): {relevance_counts[2]} ä¸ª")
        print(f"  éƒ¨åˆ†ç›¸å…³ (1): {relevance_counts[1]} ä¸ª")
        print(f"  ä¸ç›¸å…³ (0): {relevance_counts[0]} ä¸ª")
        
        relevant_count = relevance_counts[1] + relevance_counts[2] + relevance_counts[3]
        if len(results) > 0:
            precision = relevant_count / len(results)
            print(f"ç²¾ç¡®ç‡ (Precision): {precision:.3f}")
    
    def batch_evaluate(self, query_ids: List[int] = None):
        """æ‰¹é‡è¯„ä»·å¤šä¸ªæŸ¥è¯¢"""
        if query_ids is None:
            query_ids = [q.query_id for q in self.query_manager.get_all_queries()]
        
        print(f"å¼€å§‹æ‰¹é‡è¯„ä»· {len(query_ids)} ä¸ªæŸ¥è¯¢...")
        
        completed = 0
        for query_id in query_ids:
            print(f"\nè¿›åº¦: {completed + 1}/{len(query_ids)}")
            
            if self.evaluate_query(query_id):
                completed += 1
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if completed < len(query_ids):
                response = input(f"\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢ï¼Œè¾“å…¥'q'é€€å‡ºæ‰¹é‡è¯„ä»·: ").strip()
                if response.lower() == 'q':
                    break
        
        print(f"\næ‰¹é‡è¯„ä»·å®Œæˆï¼Œå…±è¯„ä»·äº† {completed} ä¸ªæŸ¥è¯¢")
    
    def interactive_evaluation(self):
        """äº¤äº’å¼è¯„ä»·ç•Œé¢"""
        self.load_or_create_queries()
        
        print(f"\n{'='*80}")
        print(f"ğŸ” äººå·¥è¯„ä»·ç³»ç»Ÿ")
        print(f"{'='*80}")
        print(f"å¯ç”¨å‘½ä»¤:")
        print(f"  list - æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•æŸ¥è¯¢")
        print(f"  eval <query_id> - è¯„ä»·æŒ‡å®šæŸ¥è¯¢")
        print(f"  batch [query_ids] - æ‰¹é‡è¯„ä»·")
        print(f"  summary - æ˜¾ç¤ºè¯„ä»·ç»Ÿè®¡")
        print(f"  export - å¯¼å‡ºè¯„ä»·ç»“æœ")
        print(f"  help - æ˜¾ç¤ºå¸®åŠ©")
        print(f"  quit - é€€å‡º")
        
        while True:
            try:
                command = input(f"\nğŸ“ è¯·è¾“å…¥å‘½ä»¤: ").strip().lower()
                
                if command == 'quit':
                    print("å†è§ï¼")
                    break
                
                elif command == 'list':
                    self.query_manager.display_queries()
                
                elif command.startswith('eval '):
                    try:
                        query_id = int(command.split()[1])
                        self.evaluate_query(query_id)
                    except (IndexError, ValueError):
                        print("ç”¨æ³•: eval <query_id>")
                
                elif command == 'batch':
                    self.batch_evaluate()
                
                elif command.startswith('batch '):
                    try:
                        query_ids = [int(x) for x in command.split()[1:]]
                        self.batch_evaluate(query_ids)
                    except ValueError:
                        print("ç”¨æ³•: batch [query_id1 query_id2 ...]")
                
                elif command == 'summary':
                    self._show_overall_summary()
                
                elif command == 'export':
                    self._export_results()
                
                elif command == 'help':
                    self._show_help()
                
                elif command:
                    print("æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥'help'æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
                    
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {e}")
    
    def _show_overall_summary(self):
        """æ˜¾ç¤ºæ€»ä½“è¯„ä»·ç»Ÿè®¡"""
        if not self.current_evaluations:
            self._load_evaluation()
        
        print(f"\n{'='*60}")
        print(f"æ€»ä½“è¯„ä»·ç»Ÿè®¡")
        print(f"{'='*60}")
        
        total_evaluated = len(self.current_evaluations)
        total_results = sum(len(eval_data['results_evaluated']) 
                          for eval_data in self.current_evaluations.values())
        
        all_scores = []
        for eval_data in self.current_evaluations.values():
            for result in eval_data['results_evaluated']:
                all_scores.append(result['relevance_score'])
        
        if all_scores:
            relevance_dist = {0: 0, 1: 0, 2: 0, 3: 0}
            for score in all_scores:
                relevance_dist[score] += 1
            
            print(f"å·²è¯„ä»·æŸ¥è¯¢æ•°: {total_evaluated}")
            print(f"å·²è¯„ä»·ç»“æœæ€»æ•°: {total_results}")
            print(f"å¹³å‡æ¯æŸ¥è¯¢è¯„ä»·ç»“æœæ•°: {total_results/total_evaluated:.1f}")
            print(f"\nç›¸å…³æ€§åˆ†å¸ƒ:")
            for score, count in relevance_dist.items():
                pct = count / len(all_scores) * 100
                print(f"  è¯„åˆ† {score}: {count} ä¸ª ({pct:.1f}%)")
            
            relevant_count = sum(relevance_dist[i] for i in [1, 2, 3])
            overall_precision = relevant_count / len(all_scores)
            print(f"\næ€»ä½“ç²¾ç¡®ç‡: {overall_precision:.3f}")
        else:
            print("æš‚æ— è¯„ä»·æ•°æ®")
    
    def _export_results(self):
        """å¯¼å‡ºè¯„ä»·ç»“æœ"""
        if not self.current_evaluations:
            self._load_evaluation()
        
        export_file = "results/evaluation_export.json"
        os.makedirs(os.path.dirname(export_file), exist_ok=True)
        
        export_data = {
            'timestamp': time.time(),
            'total_queries_evaluated': len(self.current_evaluations),
            'evaluations': self.current_evaluations,
            'summary': self._calculate_summary_stats()
        }
        
        with open(export_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"è¯„ä»·ç»“æœå·²å¯¼å‡ºåˆ°: {export_file}")
    
    def _calculate_summary_stats(self) -> Dict[str, Any]:
        """è®¡ç®—æ€»ç»“ç»Ÿè®¡"""
        if not self.current_evaluations:
            return {}
        
        all_scores = []
        query_precisions = []
        
        for eval_data in self.current_evaluations.values():
            scores = [r['relevance_score'] for r in eval_data['results_evaluated']]
            all_scores.extend(scores)
            
            if scores:
                relevant = sum(1 for s in scores if s > 0)
                precision = relevant / len(scores)
                query_precisions.append(precision)
        
        if all_scores:
            return {
                'total_results_evaluated': len(all_scores),
                'overall_precision': sum(1 for s in all_scores if s > 0) / len(all_scores),
                'average_query_precision': sum(query_precisions) / len(query_precisions),
                'relevance_distribution': {
                    str(i): all_scores.count(i) for i in range(4)
                }
            }
        return {}
    
    def _save_evaluation(self):
        """ä¿å­˜è¯„ä»·ç»“æœ"""
        os.makedirs(os.path.dirname(self.evaluation_file), exist_ok=True)
        
        # ä¿å­˜è¯„ä»·ç»“æœåˆ°manual_evaluation.json
        with open(self.evaluation_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_evaluations, f, indent=2, ensure_ascii=False)
        
        # æ›´æ–°test_queries.json
        self.query_manager.save_queries()
        
        print("è¯„ä»·ç»“æœå·²ä¿å­˜")
    
    def _load_evaluation(self):
        """åŠ è½½è¯„ä»·ç»“æœ"""
        if os.path.exists(self.evaluation_file):
            try:
                with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                    self.current_evaluations = json.load(f)
                print(f"åŠ è½½äº† {len(self.current_evaluations)} ä¸ªæŸ¥è¯¢çš„è¯„ä»·ç»“æœ")
            except Exception as e:
                print(f"åŠ è½½è¯„ä»·æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"äººå·¥è¯„ä»·ç³»ç»Ÿå¸®åŠ©")
        print(f"{'='*60}")
        print(f"å‘½ä»¤è¯´æ˜:")
        print(f"  list - æ˜¾ç¤ºæ‰€æœ‰å¯è¯„ä»·çš„æµ‹è¯•æŸ¥è¯¢")
        print(f"  eval <query_id> - å¯¹æŒ‡å®šIDçš„æŸ¥è¯¢è¿›è¡Œè¯„ä»·")
        print(f"  batch - å¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œæ‰¹é‡è¯„ä»·")
        print(f"  batch <id1 id2...> - å¯¹æŒ‡å®šæŸ¥è¯¢è¿›è¡Œæ‰¹é‡è¯„ä»·")
        print(f"  summary - æ˜¾ç¤ºå½“å‰è¯„ä»·ç»Ÿè®¡ä¿¡æ¯")
        print(f"  export - å°†è¯„ä»·ç»“æœå¯¼å‡ºä¸ºJSONæ–‡ä»¶")
        print(f"  help - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print(f"  quit - é€€å‡ºè¯„ä»·ç³»ç»Ÿ")
        print(f"\nè¯„ä»·æ—¶çš„è¯„åˆ†æ ‡å‡†:")
        print(f"  3 - éå¸¸ç›¸å…³: å®Œå…¨ç¬¦åˆæŸ¥è¯¢æ„å›¾")
        print(f"  2 - ç›¸å…³: ä¸æŸ¥è¯¢æœ‰æ˜ç¡®å…³è”")
        print(f"  1 - éƒ¨åˆ†ç›¸å…³: æœ‰ä¸€å®šå…³è”ä½†ä¸å¤ªç¬¦åˆ")
        print(f"  0 - ä¸ç›¸å…³: ä¸æŸ¥è¯¢å®Œå…¨æ— å…³")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== äººå·¥è¯„ä»·å·¥å…·æµ‹è¯• ===")
    print("æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦å®Œæ•´çš„æœç´¢å¼•æ“")
    print("è¯·è¿è¡Œå®Œæ•´çš„è¯„ä»·ç³»ç»Ÿï¼š")
    print("python src/evaluation/manual_evaluation.py")