#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿¡æ¯æŠ½å–è¯„ä»·å™¨
æ–‡ä»¶ä½ç½®ï¼šsrc/evaluation/extraction_evaluator.py
"""

import json
import random
import os
from typing import List, Dict, Any, Optional
from collections import defaultdict


class MockDocument:
    """æ¨¡æ‹Ÿæ–‡æ¡£å¯¹è±¡ï¼ˆä¸ç°æœ‰æµ‹è¯•ä»£ç å…¼å®¹ï¼‰"""

    def __init__(self, doc_id: int, title: str, content: str, summary: str = ""):
        self.doc_id = doc_id
        self.title = title
        self.content = content
        self.summary = summary


class ExtractionEvaluator:
    """ä¿¡æ¯æŠ½å–äººå·¥è¯„ä»·å™¨"""

    def __init__(self, extraction_manager, config: Dict[str, Any]):
        self.extraction_manager = extraction_manager
        self.config = config
        self.evaluation_data = {}

    def create_sample(self, articles: List[Dict]) -> List[MockDocument]:
        """åˆ›å»ºè¯„ä»·æ ·æœ¬"""
        sample_size = self.config.get('sample_size', 50)

        # éšæœºé‡‡æ ·
        if len(articles) > sample_size:
            sampled_articles = random.sample(articles, sample_size)
        else:
            sampled_articles = articles

        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        sample_docs = []
        for i, article in enumerate(sampled_articles):
            doc = MockDocument(
                doc_id=i,
                title=article.get('title', ''),
                content=article.get('content', ''),
                summary=article.get('summary', '')
            )
            sample_docs.append(doc)

        # ä¿å­˜æ ·æœ¬ä¿¡æ¯
        sample_info = {
            'sample_size': len(sample_docs),
            'documents': [
                {
                    'doc_id': doc.doc_id,
                    'title': doc.title,
                    'content_preview': doc.content[:200] + '...' if len(doc.content) > 200 else doc.content
                }
                for doc in sample_docs
            ]
        }

        sample_file = self.config.get('sample_file', 'results/evaluation_sample.json')
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_info, f, indent=2, ensure_ascii=False)

        return sample_docs

    def extract_from_sample(self, sample_docs: List[MockDocument]) -> Dict[int, List]:
        """å¯¹æ ·æœ¬æ–‡æ¡£è¿›è¡Œä¿¡æ¯æŠ½å–"""
        results = {}

        print("ğŸ“Š æ­£åœ¨å¯¹æ ·æœ¬æ–‡æ¡£è¿›è¡Œä¿¡æ¯æŠ½å–...")

        for i, doc in enumerate(sample_docs):
            print(f"  å¤„ç†æ–‡æ¡£ {i + 1}/{len(sample_docs)}: {doc.title[:50]}...")

            # æ„å»ºå®Œæ•´æ–‡æœ¬
            full_text = f"{doc.title}. {doc.summary} {doc.content}"

            # æ‰§è¡ŒæŠ½å–
            entities = self.extraction_manager.extract_from_text(full_text, doc.doc_id, "evaluation")

            # é™åˆ¶æ¯ç¯‡æ–‡æ¡£çš„å®ä½“æ•°é‡
            max_entities = self.config.get('entities_per_doc', 10)
            if len(entities) > max_entities:
                # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰Nä¸ª
                entities = sorted(entities, key=lambda x: x.confidence, reverse=True)[:max_entities]

            results[doc.doc_id] = entities

        # ä¿å­˜æŠ½å–ç»“æœ
        sample_results = {}
        for doc_id, entities in results.items():
            sample_results[doc_id] = {
                'document_info': {
                    'doc_id': doc_id,
                    'title': sample_docs[doc_id].title,
                    'content_preview': sample_docs[doc_id].content[:300]
                },
                'extracted_entities': [
                    {
                        'entity_type': entity.entity_type,
                        'entity_value': entity.entity_value,
                        'confidence': entity.confidence,
                        'field': entity.field,
                        'context': entity.context[:100] if entity.context else '',
                        'start_pos': getattr(entity, 'start_pos', -1),
                        'end_pos': getattr(entity, 'end_pos', -1)
                    }
                    for entity in entities
                ]
            }

        # ä¿å­˜åˆ°æ–‡ä»¶
        sample_file = self.config.get('sample_file', 'results/evaluation_sample.json')
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_results, f, indent=2, ensure_ascii=False)

        return sample_results

    def load_sample_results(self) -> Optional[Dict]:
        """åŠ è½½å·²æœ‰çš„æ ·æœ¬æŠ½å–ç»“æœ"""
        sample_file = self.config.get('sample_file', 'results/evaluation_sample.json')

        if not os.path.exists(sample_file):
            return None

        try:
            with open(sample_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æ ·æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def start_manual_evaluation(self, sample_results: Dict[int, Dict]):
        """å¼€å§‹äººå·¥è¯„ä»·"""
        eval_file = self.config.get('evaluation_file', 'results/extraction_evaluation.json')

        # åŠ è½½å·²æœ‰çš„è¯„ä»·ç»“æœ
        if os.path.exists(eval_file):
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    self.evaluation_data = json.load(f)
                print(f"ğŸ“‹ å·²åŠ è½½ç°æœ‰è¯„ä»·æ•°æ®ï¼ŒåŒ…å« {len(self.evaluation_data)} ç¯‡æ–‡æ¡£çš„è¯„ä»·")
            except:
                self.evaluation_data = {}
        else:
            self.evaluation_data = {}

        # å¼€å§‹è¯„ä»·æµç¨‹
        docs_to_evaluate = list(sample_results.keys())
        evaluated_count = 0

        for doc_id in docs_to_evaluate:
            doc_id_str = str(doc_id)

            # æ£€æŸ¥æ˜¯å¦å·²ç»è¯„ä»·è¿‡
            if doc_id_str in self.evaluation_data:
                response = input(f"\næ–‡æ¡£ {doc_id} å·²è¯„ä»·è¿‡ï¼Œæ˜¯å¦é‡æ–°è¯„ä»·ï¼Ÿ(y/n): ").strip().lower()
                if response != 'y':
                    continue

            # è¯„ä»·å•ä¸ªæ–‡æ¡£
            if self._evaluate_document(doc_id, sample_results[doc_id]):
                evaluated_count += 1

                # å®šæœŸä¿å­˜
                if evaluated_count % 5 == 0:
                    self._save_evaluation_data()
                    print(f"âœ… å·²ä¿å­˜è¯„ä»·è¿›åº¦ ({evaluated_count} ç¯‡æ–‡æ¡£)")

            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if doc_id != docs_to_evaluate[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ–‡æ¡£
                response = input(f"\nç»§ç»­è¯„ä»·ä¸‹ä¸€ç¯‡æ–‡æ¡£ï¼Ÿ(y/n/qé€€å‡º): ").strip().lower()
                if response == 'q':
                    break
                elif response == 'n':
                    break

        # æœ€ç»ˆä¿å­˜
        self._save_evaluation_data()
        print(f"\nâœ… è¯„ä»·å®Œæˆï¼å…±è¯„ä»·äº† {evaluated_count} ç¯‡æ–‡æ¡£")

    def _evaluate_document(self, doc_id: int, doc_data: Dict) -> bool:
        """è¯„ä»·å•ä¸ªæ–‡æ¡£"""
        print(f"\n{'=' * 80}")
        print(f"ğŸ“„ æ–‡æ¡£ {doc_id}: {doc_data['document_info']['title']}")
        print(f"{'=' * 80}")

        # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹é¢„è§ˆ
        content_preview = doc_data['document_info']['content_preview']
        print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:")
        print(f"{content_preview}...")

        extracted_entities = doc_data['extracted_entities']
        print(f"\nğŸ” æŠ½å–åˆ° {len(extracted_entities)} ä¸ªå®ä½“")

        if not extracted_entities:
            print("âŒ è¯¥æ–‡æ¡£æ²¡æœ‰æŠ½å–åˆ°ä»»ä½•å®ä½“")
            return False

        # è¯„ä»·æ¯ä¸ªæŠ½å–çš„å®ä½“
        evaluated_entities = []

        for i, entity in enumerate(extracted_entities):
            print(f"\n--- å®ä½“ {i + 1}/{len(extracted_entities)} ---")
            print(f"ç±»å‹: {entity['entity_type']}")
            print(f"å€¼: {entity['entity_value']}")
            print(f"ç½®ä¿¡åº¦: {entity['confidence']:.3f}")
            print(f"å­—æ®µ: {entity['field']}")
            if entity['context']:
                print(f"ä¸Šä¸‹æ–‡: {entity['context']}")

            while True:
                try:
                    evaluation = input("è¯„ä»· [âœ“(y)æ­£ç¡® / âœ—(n)é”™è¯¯ / ?(s)è·³è¿‡ / qé€€å‡º]: ").strip().lower()

                    if evaluation == 'q':
                        return False
                    elif evaluation in ['y', 'yes', 'âœ“', '1']:
                        evaluated_entities.append({
                            **entity,
                            'human_evaluation': 'correct',
                            'evaluation_notes': ''
                        })
                        print("âœ… æ ‡è®°ä¸ºæ­£ç¡®")
                        break
                    elif evaluation in ['n', 'no', 'âœ—', '0']:
                        notes = input("  è¯·è¯´æ˜é”™è¯¯åŸå› ï¼ˆå¯é€‰ï¼‰: ").strip()
                        evaluated_entities.append({
                            **entity,
                            'human_evaluation': 'incorrect',
                            'evaluation_notes': notes
                        })
                        print("âŒ æ ‡è®°ä¸ºé”™è¯¯")
                        break
                    elif evaluation in ['s', 'skip', '?']:
                        evaluated_entities.append({
                            **entity,
                            'human_evaluation': 'skipped',
                            'evaluation_notes': ''
                        })
                        print("â­ï¸ è·³è¿‡")
                        break
                    else:
                        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ y/n/s/q")
                except KeyboardInterrupt:
                    print("\nâŒ è¯„ä»·è¢«ä¸­æ–­")
                    return False

        # è¯¢é—®é—æ¼çš„å®ä½“
        print(f"\nğŸ” æ˜¯å¦æœ‰é—æ¼çš„å®ä½“ï¼Ÿ")
        print("è¯·è¾“å…¥é—æ¼çš„å®ä½“ï¼Œæ ¼å¼ï¼šç±»å‹:å€¼ ï¼ˆå¦‚ PERSON:å¼ ä¸‰ï¼‰")
        print("è¾“å…¥ 'done' å®Œæˆï¼Œ'skip' è·³è¿‡")

        missed_entities = []
        while True:
            try:
                missed_input = input("é—æ¼å®ä½“: ").strip()

                if missed_input.lower() in ['done', 'finish', 'å®Œæˆ']:
                    break
                elif missed_input.lower() in ['skip', 'è·³è¿‡']:
                    break
                elif ':' in missed_input:
                    try:
                        entity_type, entity_value = missed_input.split(':', 1)
                        entity_type = entity_type.strip().upper()
                        entity_value = entity_value.strip()

                        if entity_type and entity_value:
                            missed_entities.append({
                                'entity_type': entity_type,
                                'entity_value': entity_value,
                                'human_added': True
                            })
                            print(f"âœ… å·²æ·»åŠ é—æ¼å®ä½“: {entity_type}:{entity_value}")
                    except:
                        print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ ç±»å‹:å€¼ çš„æ ¼å¼")
                else:
                    print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ ç±»å‹:å€¼ çš„æ ¼å¼")
            except KeyboardInterrupt:
                print("\nè¯„ä»·è¢«ä¸­æ–­")
                break

        # ä¿å­˜è¯„ä»·ç»“æœ
        doc_evaluation = {
            'document_info': doc_data['document_info'],
            'extraction_timestamp': doc_data.get('extraction_timestamp', ''),
            'evaluated_entities': evaluated_entities,
            'missed_entities': missed_entities,
            'evaluation_summary': {
                'total_extracted': len(extracted_entities),
                'evaluated_count': len([e for e in evaluated_entities if e['human_evaluation'] != 'skipped']),
                'correct_count': len([e for e in evaluated_entities if e['human_evaluation'] == 'correct']),
                'incorrect_count': len([e for e in evaluated_entities if e['human_evaluation'] == 'incorrect']),
                'missed_count': len(missed_entities)
            }
        }

        self.evaluation_data[str(doc_id)] = doc_evaluation

        # æ˜¾ç¤ºæœ¬æ–‡æ¡£è¯„ä»·æ‘˜è¦
        summary = doc_evaluation['evaluation_summary']
        print(f"\nğŸ“Š æ–‡æ¡£ {doc_id} è¯„ä»·æ‘˜è¦:")
        print(f"  æŠ½å–å®ä½“: {summary['total_extracted']} ä¸ª")
        print(f"  å·²è¯„ä»·: {summary['evaluated_count']} ä¸ª")
        print(f"  æ­£ç¡®: {summary['correct_count']} ä¸ª")
        print(f"  é”™è¯¯: {summary['incorrect_count']} ä¸ª")
        print(f"  é—æ¼: {summary['missed_count']} ä¸ª")

        return True

    def _save_evaluation_data(self):
        """ä¿å­˜è¯„ä»·æ•°æ®"""
        eval_file = self.config.get('evaluation_file', 'results/extraction_evaluation.json')

        try:
            with open(eval_file, 'w', encoding='utf-8') as f:
                json.dump(self.evaluation_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜è¯„ä»·æ•°æ®å¤±è´¥: {e}")

    def load_evaluation_results(self) -> Optional[Dict]:
        """åŠ è½½è¯„ä»·ç»“æœ"""
        eval_file = self.config.get('evaluation_file', 'results/extraction_evaluation.json')

        if not os.path.exists(eval_file):
            return None

        try:
            with open(eval_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½è¯„ä»·ç»“æœå¤±è´¥: {e}")
            return None