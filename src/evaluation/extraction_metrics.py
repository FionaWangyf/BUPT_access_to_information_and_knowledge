#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿¡æ¯æŠ½å–è¯„ä»·æŒ‡æ ‡è®¡ç®—å™¨
æ–‡ä»¶ä½ç½®ï¼šsrc/evaluation/extraction_metrics.py
"""

from typing import Dict, List, Any, Tuple
from collections import defaultdict
import statistics


class ExtractionMetrics:
    """ä¿¡æ¯æŠ½å–è¯„ä»·æŒ‡æ ‡è®¡ç®—å™¨"""

    def __init__(self):
        self.entity_types = ['PERSON', 'LOCATION', 'ORGANIZATION', 'TIME', 'MONEY', 'CONTACT', 'QUOTE']

    def calculate_overall_metrics(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€»ä½“è¯„ä»·æŒ‡æ ‡"""

        # æ”¶é›†æ‰€æœ‰è¯„ä»·æ•°æ®
        all_correct = 0
        all_incorrect = 0
        all_missed = 0
        all_extracted = 0

        by_type_stats = defaultdict(lambda: {
            'correct': 0, 'incorrect': 0, 'missed': 0, 'extracted': 0
        })

        confidence_scores = []
        confidence_correct = []
        confidence_incorrect = []

        for doc_id, doc_data in evaluation_data.items():
            # å¤„ç†è¯„ä»·è¿‡çš„å®ä½“
            for entity in doc_data.get('evaluated_entities', []):
                entity_type = entity['entity_type']
                confidence = entity['confidence']
                evaluation = entity['human_evaluation']

                all_extracted += 1
                by_type_stats[entity_type]['extracted'] += 1
                confidence_scores.append(confidence)

                if evaluation == 'correct':
                    all_correct += 1
                    by_type_stats[entity_type]['correct'] += 1
                    confidence_correct.append(confidence)
                elif evaluation == 'incorrect':
                    all_incorrect += 1
                    by_type_stats[entity_type]['incorrect'] += 1
                    confidence_incorrect.append(confidence)

            # å¤„ç†é—æ¼çš„å®ä½“
            for missed_entity in doc_data.get('missed_entities', []):
                entity_type = missed_entity['entity_type']
                all_missed += 1
                by_type_stats[entity_type]['missed'] += 1

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._calculate_prf_metrics(all_correct, all_incorrect, all_missed)
        overall_metrics.update({
            'total_extracted': all_extracted,
            'correct_entities': all_correct,
            'incorrect_entities': all_incorrect,
            'missed_entities': all_missed,
            'evaluated_documents': len(evaluation_data)
        })

        # è®¡ç®—å„ç±»å‹æŒ‡æ ‡
        by_type_metrics = {}
        for entity_type, stats in by_type_stats.items():
            type_metrics = self._calculate_prf_metrics(
                stats['correct'], stats['incorrect'], stats['missed']
            )
            type_metrics.update({
                'extracted_count': stats['extracted'],
                'correct_count': stats['correct'],
                'incorrect_count': stats['incorrect'],
                'missed_count': stats['missed']
            })
            by_type_metrics[entity_type] = type_metrics

        # ç½®ä¿¡åº¦åˆ†æ
        confidence_analysis = self._analyze_confidence(
            confidence_scores, confidence_correct, confidence_incorrect
        )

        # é”™è¯¯åˆ†æ
        error_analysis = self._analyze_errors(evaluation_data)

        return {
            'overall': overall_metrics,
            'by_type': by_type_metrics,
            'confidence_analysis': confidence_analysis,
            'error_analysis': error_analysis
        }

    def _calculate_prf_metrics(self, correct: int, incorrect: int, missed: int) -> Dict[str, float]:
        """è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°"""

        # è®¡ç®—ç²¾ç¡®ç‡ = æ­£ç¡®æŠ½å–çš„å®ä½“æ•° / æ€»æŠ½å–å®ä½“æ•°
        total_extracted = correct + incorrect
        precision = correct / total_extracted if total_extracted > 0 else 0.0

        # è®¡ç®—å¬å›ç‡ = æ­£ç¡®æŠ½å–çš„å®ä½“æ•° / (æ­£ç¡®æŠ½å–çš„å®ä½“æ•° + é—æ¼çš„å®ä½“æ•°)
        total_should_extract = correct + missed
        recall = correct / total_should_extract if total_should_extract > 0 else 0.0

        # è®¡ç®—F1åˆ†æ•° = 2 * (ç²¾ç¡®ç‡ * å¬å›ç‡) / (ç²¾ç¡®ç‡ + å¬å›ç‡)
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score
        }

    def _analyze_confidence(self, all_scores: List[float],
                            correct_scores: List[float],
                            incorrect_scores: List[float]) -> Dict[str, Any]:
        """åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ"""

        if not all_scores:
            return {}

        analysis = {
            'average_confidence': statistics.mean(all_scores),
            'median_confidence': statistics.median(all_scores),
            'min_confidence': min(all_scores),
            'max_confidence': max(all_scores)
        }

        if correct_scores:
            analysis['average_confidence_correct'] = statistics.mean(correct_scores)

        if incorrect_scores:
            analysis['average_confidence_incorrect'] = statistics.mean(incorrect_scores)

        # ç½®ä¿¡åº¦åŒºé—´åˆ†æ
        high_conf = [s for s in all_scores if s > 0.8]
        medium_conf = [s for s in all_scores if 0.5 <= s <= 0.8]
        low_conf = [s for s in all_scores if s < 0.5]

        analysis.update({
            'high_confidence_count': len(high_conf),
            'medium_confidence_count': len(medium_conf),
            'low_confidence_count': len(low_conf),
            'high_confidence_ratio': len(high_conf) / len(all_scores),
            'medium_confidence_ratio': len(medium_conf) / len(all_scores),
            'low_confidence_ratio': len(low_conf) / len(all_scores)
        })

        return analysis

    def _analyze_errors(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯ç±»å‹å’Œæ¨¡å¼"""

        error_types = defaultdict(int)
        error_examples = defaultdict(list)
        missed_by_type = defaultdict(int)

        for doc_id, doc_data in evaluation_data.items():
            # åˆ†æé”™è¯¯çš„æŠ½å–
            for entity in doc_data.get('evaluated_entities', []):
                if entity['human_evaluation'] == 'incorrect':
                    entity_type = entity['entity_type']
                    error_types[entity_type] += 1

                    error_info = {
                        'entity_value': entity['entity_value'],
                        'confidence': entity['confidence'],
                        'context': entity.get('context', ''),
                        'notes': entity.get('evaluation_notes', ''),
                        'doc_id': doc_id
                    }
                    error_examples[entity_type].append(error_info)

            # åˆ†æé—æ¼çš„å®ä½“
            for missed_entity in doc_data.get('missed_entities', []):
                entity_type = missed_entity['entity_type']
                missed_by_type[entity_type] += 1

        return {
            'error_count_by_type': dict(error_types),
            'missed_count_by_type': dict(missed_by_type),
            'error_examples': dict(error_examples),
            'total_errors': sum(error_types.values()),
            'total_missed': sum(missed_by_type.values())
        }

    def generate_performance_summary(self, metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""

        overall = metrics.get('overall', {})
        by_type = metrics.get('by_type', {})
        confidence = metrics.get('confidence_analysis', {})
        errors = metrics.get('error_analysis', {})

        summary = []
        summary.append("=" * 60)
        summary.append("ä¿¡æ¯æŠ½å–ç³»ç»Ÿæ€§èƒ½è¯„ä»·æŠ¥å‘Š")
        summary.append("=" * 60)

        # æ€»ä½“æ€§èƒ½
        summary.append(f"\nğŸ¯ æ€»ä½“æ€§èƒ½:")
        summary.append(f"  ç²¾ç¡®ç‡: {overall.get('precision', 0):.1%}")
        summary.append(f"  å¬å›ç‡: {overall.get('recall', 0):.1%}")
        summary.append(f"  F1åˆ†æ•°: {overall.get('f1_score', 0):.1%}")
        summary.append(f"  è¯„ä»·æ–‡æ¡£æ•°: {overall.get('evaluated_documents', 0)}")
        summary.append(f"  æŠ½å–å®ä½“æ€»æ•°: {overall.get('total_extracted', 0)}")
        summary.append(f"  æ­£ç¡®å®ä½“æ•°: {overall.get('correct_entities', 0)}")
        summary.append(f"  é”™è¯¯å®ä½“æ•°: {overall.get('incorrect_entities', 0)}")
        summary.append(f"  é—æ¼å®ä½“æ•°: {overall.get('missed_entities', 0)}")

        # å„ç±»å‹æ€§èƒ½
        if by_type:
            summary.append(f"\nğŸ“Š å„å®ä½“ç±»å‹æ€§èƒ½:")
            summary.append(f"{'ç±»å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8}")
            summary.append("-" * 50)

            for entity_type, type_metrics in by_type.items():
                precision = type_metrics.get('precision', 0)
                recall = type_metrics.get('recall', 0)
                f1 = type_metrics.get('f1_score', 0)
                summary.append(f"{entity_type:<12} {precision:<8.1%} {recall:<8.1%} {f1:<8.1%}")

        # ç½®ä¿¡åº¦åˆ†æ
        if confidence:
            summary.append(f"\nğŸšï¸ ç½®ä¿¡åº¦åˆ†æ:")
            summary.append(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence.get('average_confidence', 0):.3f}")
            summary.append(f"  é«˜ç½®ä¿¡åº¦å®ä½“æ¯”ä¾‹ (>0.8): {confidence.get('high_confidence_ratio', 0):.1%}")
            summary.append(f"  ä¸­ç½®ä¿¡åº¦å®ä½“æ¯”ä¾‹ (0.5-0.8): {confidence.get('medium_confidence_ratio', 0):.1%}")
            summary.append(f"  ä½ç½®ä¿¡åº¦å®ä½“æ¯”ä¾‹ (<0.5): {confidence.get('low_confidence_ratio', 0):.1%}")

            if 'average_confidence_correct' in confidence and 'average_confidence_incorrect' in confidence:
                summary.append(f"  æ­£ç¡®å®ä½“å¹³å‡ç½®ä¿¡åº¦: {confidence['average_confidence_correct']:.3f}")
                summary.append(f"  é”™è¯¯å®ä½“å¹³å‡ç½®ä¿¡åº¦: {confidence['average_confidence_incorrect']:.3f}")

        # é”™è¯¯åˆ†æ
        if errors:
            summary.append(f"\nâŒ é”™è¯¯åˆ†æ:")
            summary.append(f"  æ€»é”™è¯¯æ•°: {errors.get('total_errors', 0)}")
            summary.append(f"  æ€»é—æ¼æ•°: {errors.get('total_missed', 0)}")

            error_by_type = errors.get('error_count_by_type', {})
            if error_by_type:
                summary.append(f"  å„ç±»å‹é”™è¯¯åˆ†å¸ƒ:")
                for entity_type, count in error_by_type.items():
                    summary.append(f"    {entity_type}: {count} ä¸ª")

            missed_by_type = errors.get('missed_count_by_type', {})
            if missed_by_type:
                summary.append(f"  å„ç±»å‹é—æ¼åˆ†å¸ƒ:")
                for entity_type, count in missed_by_type.items():
                    summary.append(f"    {entity_type}: {count} ä¸ª")

        summary.append("=" * 60)

        return "\n".join(summary)

    def calculate_confidence_threshold_performance(self, evaluation_data: Dict[str, Any],
                                                   thresholds: List[float] = None) -> Dict[float, Dict[str, float]]:
        """è®¡ç®—ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ä¸‹çš„æ€§èƒ½"""

        if thresholds is None:
            thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

        threshold_performance = {}

        for threshold in thresholds:
            # ç­›é€‰é«˜äºé˜ˆå€¼çš„å®ä½“
            filtered_correct = 0
            filtered_incorrect = 0
            all_missed = 0

            for doc_id, doc_data in evaluation_data.items():
                for entity in doc_data.get('evaluated_entities', []):
                    if entity['confidence'] >= threshold:
                        if entity['human_evaluation'] == 'correct':
                            filtered_correct += 1
                        elif entity['human_evaluation'] == 'incorrect':
                            filtered_incorrect += 1

                # é—æ¼çš„å®ä½“æ•°é‡ä¸å˜
                all_missed += len(doc_data.get('missed_entities', []))

            # è®¡ç®—è¯¥é˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
            metrics = self._calculate_prf_metrics(filtered_correct, filtered_incorrect, all_missed)
            metrics['total_extracted'] = filtered_correct + filtered_incorrect

            threshold_performance[threshold] = metrics

        return threshold_performance