"""
ä¿¡æ¯æŠ½å–ç®¡ç†å™¨
åè°ƒå¤šä¸ªæŠ½å–å™¨ï¼Œç®¡ç†æŠ½å–æµç¨‹ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£
æ–‡ä»¶ä½ç½®ï¼šsrc/extraction/extraction_manager.py
"""

import time
import json
import os
from typing import List, Dict, Any, Optional, Union
from collections import defaultdict
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.extraction.extractor_base import BaseExtractor, ExtractionResult
from src.extraction.regex_extractor import RegexExtractor

class ExtractionManager:
    """ä¿¡æ¯æŠ½å–ç®¡ç†å™¨ï¼šæ•´åˆå¤šä¸ªæŠ½å–å™¨ï¼Œç®¡ç†æŠ½å–æµç¨‹"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æŠ½å–ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config or self._get_default_config()
        self.extractors = {}  # æŠ½å–å™¨é›†åˆ
        self.is_initialized = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_documents_processed': 0,
            'total_extractions': 0,
            'total_processing_time': 0.0,
            'extractions_by_type': defaultdict(int),
            'extractions_by_extractor': defaultdict(int),
            'average_confidence': 0.0,
            'documents_with_extractions': 0
        }
        
        # ç»“æœç¼“å­˜
        self.results_cache = {}
        self.enable_cache = self.config.get('enable_cache', True)
        
        print(f"ğŸ”§ åˆå§‹åŒ–æŠ½å–ç®¡ç†å™¨")
        print(f"âš™ï¸ é…ç½®: {self.config}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'enable_regex_extractor': True,
            'regex_confidence_threshold': 0.6,
            'enable_cache': True,
            'merge_duplicate_entities': True,
            'min_entity_confidence': 0.5,
            'max_entities_per_type': 100,
            'enable_post_processing': True,
            'save_detailed_stats': True
        }
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æŠ½å–å™¨"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–æŠ½å–ç®¡ç†å™¨...")
            
            # åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨
            if self.config.get('enable_regex_extractor', True):
                regex_threshold = self.config.get('regex_confidence_threshold', 0.6)
                regex_extractor = RegexExtractor(confidence_threshold=regex_threshold)
                
                if regex_extractor.initialize():
                    self.extractors['regex'] = regex_extractor
                    print(f"âœ… æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨åˆå§‹åŒ–æˆåŠŸ (é˜ˆå€¼: {regex_threshold})")
                else:
                    print(f"âŒ æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨åˆå§‹åŒ–å¤±è´¥")
                    return False
            
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–æŠ½å–å™¨
            # if self.config.get('enable_ml_extractor', False):
            #     ml_extractor = MLExtractor()
            #     if ml_extractor.initialize():
            #         self.extractors['ml'] = ml_extractor
            
            if not self.extractors:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æŠ½å–å™¨")
                return False
            
            self.is_initialized = True
            print(f"âœ… æŠ½å–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(self.extractors)} ä¸ªæŠ½å–å™¨")
            return True
            
        except Exception as e:
            print(f"âŒ æŠ½å–ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def extract_from_text(self, text: str, doc_id: int, field: str = "content") -> List[ExtractionResult]:
        """
        ä»å•ä¸ªæ–‡æœ¬ä¸­æŠ½å–ä¿¡æ¯
        
        Args:
            text: å¾…æŠ½å–çš„æ–‡æœ¬
            doc_id: æ–‡æ¡£ID
            field: å­—æ®µåç§°
            
        Returns:
            æŠ½å–ç»“æœåˆ—è¡¨
        """
        if not self.is_initialized:
            print("âŒ æŠ½å–ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return []
        
        if not text or not text.strip():
            return []
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{doc_id}_{field}_{hash(text)}"
        if self.enable_cache and cache_key in self.results_cache:
            return self.results_cache[cache_key]
        
        start_time = time.time()
        all_results = []
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æŠ½å–å™¨
        for extractor_name, extractor in self.extractors.items():
            try:
                extractor_results = extractor.extract_from_text(text, doc_id, field)
                
                # ä¸ºç»“æœæ·»åŠ æŠ½å–å™¨ä¿¡æ¯
                for result in extractor_results:
                    result.metadata['extractor'] = extractor_name
                
                all_results.extend(extractor_results)
                self.stats['extractions_by_extractor'][extractor_name] += len(extractor_results)
                
            except Exception as e:
                print(f"âš ï¸ æŠ½å–å™¨ {extractor_name} å¤„ç†å¤±è´¥: {e}")
                continue
        
        # åå¤„ç†
        if self.config.get('enable_post_processing', True):
            all_results = self._post_process_results(all_results)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self._update_stats(all_results, processing_time)
        
        # ç¼“å­˜ç»“æœ
        if self.enable_cache:
            self.results_cache[cache_key] = all_results
        
        return all_results
    
    def extract_from_document(self, document: Any) -> List[ExtractionResult]:
        """
        ä»æ–‡æ¡£å¯¹è±¡ä¸­æŠ½å–ä¿¡æ¯
        
        Args:
            document: æ–‡æ¡£å¯¹è±¡ï¼ˆéœ€è¦æœ‰ doc_id, title, content ç­‰å±æ€§ï¼‰
            
        Returns:
            æŠ½å–ç»“æœåˆ—è¡¨
        """
        if not self.is_initialized:
            print("âŒ æŠ½å–ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return []
        
        all_results = []
        
        # ä»æ ‡é¢˜æŠ½å–
        if hasattr(document, 'title') and document.title:
            title_results = self.extract_from_text(
                document.title, document.doc_id, 'title'
            )
            all_results.extend(title_results)
        
        # ä»æ‘˜è¦æŠ½å–
        if hasattr(document, 'summary') and document.summary:
            summary_results = self.extract_from_text(
                document.summary, document.doc_id, 'summary'
            )
            all_results.extend(summary_results)
        
        # ä»å†…å®¹æŠ½å–
        if hasattr(document, 'content') and document.content:
            content_results = self.extract_from_text(
                document.content, document.doc_id, 'content'
            )
            all_results.extend(content_results)
        
        return all_results
    
    def extract_from_documents(self, documents: List[Any], 
                             progress_callback: Optional[callable] = None) -> Dict[int, List[ExtractionResult]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ–‡æ¡£IDåˆ°æŠ½å–ç»“æœçš„æ˜ å°„
        """
        if not self.is_initialized:
            print("âŒ æŠ½å–ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return {}
        
        print(f"ğŸ” å¼€å§‹æ‰¹é‡æŠ½å–ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        start_time = time.time()
        
        results_by_doc = {}
        
        for i, document in enumerate(documents):
            try:
                doc_results = self.extract_from_document(document)
                results_by_doc[document.doc_id] = doc_results
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress = (i + 1) / len(documents)
                    progress_callback(i + 1, len(documents), progress)
                
                # æ¯å¤„ç†100ä¸ªæ–‡æ¡£è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (i + 1)
                    remaining = (len(documents) - i - 1) * avg_time
                    print(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(documents)} ä¸ªæ–‡æ¡£ï¼Œé¢„è®¡å‰©ä½™æ—¶é—´: {remaining:.1f}ç§’")
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡æ¡£ {document.doc_id} å¤±è´¥: {e}")
                results_by_doc[document.doc_id] = []
                continue
        
        total_time = time.time() - start_time
        total_extractions = sum(len(results) for results in results_by_doc.values())
        
        print(f"âœ… æ‰¹é‡æŠ½å–å®Œæˆ:")
        print(f"  ğŸ“„ å¤„ç†æ–‡æ¡£: {len(documents)} ä¸ª")
        print(f"  ğŸ” æ€»æŠ½å–æ•°: {total_extractions} ä¸ªå®ä½“")
        print(f"  â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"  ğŸ“Š å¹³å‡æ¯æ–‡æ¡£: {total_extractions/len(documents):.1f} ä¸ªå®ä½“")
        print(f"  ğŸš€ å¤„ç†é€Ÿåº¦: {len(documents)/total_time:.1f} æ–‡æ¡£/ç§’")
        
        return results_by_doc
    
    def _post_process_results(self, results: List[ExtractionResult]) -> List[ExtractionResult]:
        """åå¤„ç†æŠ½å–ç»“æœ"""
        if not results:
            return results
        
        # 1. è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
        min_confidence = self.config.get('min_entity_confidence', 0.5)
        filtered_results = [r for r in results if r.confidence >= min_confidence]
        
        # 2. å»é‡ç›¸ä¼¼å®ä½“
        if self.config.get('merge_duplicate_entities', True):
            filtered_results = self._merge_duplicate_entities(filtered_results)
        
        # 3. é™åˆ¶æ¯ç§ç±»å‹çš„å®ä½“æ•°é‡
        max_per_type = self.config.get('max_entities_per_type', 100)
        filtered_results = self._limit_entities_per_type(filtered_results, max_per_type)
        
        # 4. æŒ‰ç½®ä¿¡åº¦æ’åº
        filtered_results.sort(key=lambda x: (-x.confidence, x.entity_type, x.start_position))
        
        return filtered_results
    
    def _merge_duplicate_entities(self, results: List[ExtractionResult]) -> List[ExtractionResult]:
        """åˆå¹¶é‡å¤çš„å®ä½“"""
        merged_results = []
        
        # æŒ‰å®ä½“ç±»å‹å’Œå€¼åˆ†ç»„
        entity_groups = defaultdict(list)
        for result in results:
            key = (result.entity_type, result.entity_value.lower().strip())
            entity_groups[key].append(result)
        
        # æ¯ç»„ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        for group in entity_groups.values():
            if len(group) == 1:
                merged_results.append(group[0])
            else:
                # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
                best_result = max(group, key=lambda x: x.confidence)
                
                # åˆå¹¶å…ƒæ•°æ®
                all_extractors = set()
                all_patterns = set()
                for result in group:
                    if 'extractor' in result.metadata:
                        all_extractors.add(result.metadata['extractor'])
                    if 'pattern_description' in result.metadata:
                        all_patterns.add(result.metadata['pattern_description'])
                
                best_result.metadata['merged_from'] = len(group)
                best_result.metadata['all_extractors'] = list(all_extractors)
                best_result.metadata['all_patterns'] = list(all_patterns)
                
                merged_results.append(best_result)
        
        return merged_results
    
    def _limit_entities_per_type(self, results: List[ExtractionResult], 
                                max_per_type: int) -> List[ExtractionResult]:
        """é™åˆ¶æ¯ç§ç±»å‹çš„å®ä½“æ•°é‡"""
        if max_per_type <= 0:
            return results
        
        # æŒ‰ç±»å‹åˆ†ç»„
        type_groups = defaultdict(list)
        for result in results:
            type_groups[result.entity_type].append(result)
        
        # æ¯ç§ç±»å‹åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„å‰Nä¸ª
        limited_results = []
        for entity_type, type_results in type_groups.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            type_results.sort(key=lambda x: x.confidence, reverse=True)
            limited_results.extend(type_results[:max_per_type])
        
        return limited_results
    
    def _update_stats(self, results: List[ExtractionResult], processing_time: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_documents_processed'] += 1
        self.stats['total_extractions'] += len(results)
        self.stats['total_processing_time'] += processing_time
        
        if results:
            self.stats['documents_with_extractions'] += 1
            
            # æ›´æ–°å„ç±»å‹ç»Ÿè®¡
            for result in results:
                self.stats['extractions_by_type'][result.entity_type] += 1
            
            # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
            total_confidence = sum(r.confidence for r in results)
            total_results = self.stats['total_extractions']
            if total_results > 0:
                self.stats['average_confidence'] = (
                    self.stats['average_confidence'] * (total_results - len(results)) + 
                    total_confidence
                ) / total_results
    
    def get_summary_statistics(self) -> Dict[str, Any]:
        """è·å–æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        stats = dict(self.stats)
        
        # æ·»åŠ è®¡ç®—æŒ‡æ ‡
        if stats['total_documents_processed'] > 0:
            stats['avg_extractions_per_document'] = (
                stats['total_extractions'] / stats['total_documents_processed']
            )
            stats['avg_processing_time_per_document'] = (
                stats['total_processing_time'] / stats['total_documents_processed']
            )
            stats['documents_with_extractions_ratio'] = (
                stats['documents_with_extractions'] / stats['total_documents_processed']
            )
        
        # è½¬æ¢ defaultdict ä¸ºæ™®é€šå­—å…¸
        stats['extractions_by_type'] = dict(stats['extractions_by_type'])
        stats['extractions_by_extractor'] = dict(stats['extractions_by_extractor'])
        
        return stats
    
    def save_statistics(self, filepath: str):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        try:
            stats = self.get_summary_statistics()
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œé…ç½®ä¿¡æ¯
            stats['timestamp'] = time.time()
            stats['config'] = self.config
            
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def export_results(self, results: Union[List[ExtractionResult], Dict[int, List[ExtractionResult]]], 
                      filepath: str, format: str = 'json'):
        """
        å¯¼å‡ºæŠ½å–ç»“æœ
        
        Args:
            results: æŠ½å–ç»“æœ
            filepath: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: è¾“å‡ºæ ¼å¼ ('json', 'csv', 'txt')
        """
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            if format == 'json':
                self._export_json(results, filepath)
            elif format == 'csv':
                self._export_csv(results, filepath)
            elif format == 'txt':
                self._export_txt(results, filepath)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
                
            print(f"ğŸ“¤ ç»“æœå·²å¯¼å‡ºåˆ°: {filepath}")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºç»“æœå¤±è´¥: {e}")
    
    def _export_json(self, results: Union[List[ExtractionResult], Dict[int, List[ExtractionResult]]], 
                    filepath: str):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        if isinstance(results, dict):
            export_data = {
                doc_id: [result.to_dict() for result in doc_results]
                for doc_id, doc_results in results.items()
            }
        else:
            export_data = [result.to_dict() for result in results]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
    
    def _export_csv(self, results: Union[List[ExtractionResult], Dict[int, List[ExtractionResult]]], 
                   filepath: str):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        import csv
        
        # å±•å¹³ç»“æœ
        if isinstance(results, dict):
            all_results = []
            for doc_results in results.values():
                all_results.extend(doc_results)
        else:
            all_results = results
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            writer.writerow([
                'doc_id', 'entity_type', 'entity_value', 'confidence',
                'start_position', 'end_position', 'field', 'context'
            ])
            
            # å†™å…¥æ•°æ®
            for result in all_results:
                writer.writerow([
                    result.doc_id, result.entity_type, result.entity_value,
                    result.confidence, result.start_position, result.end_position,
                    result.field, result.context
                ])
    
    def _export_txt(self, results: Union[List[ExtractionResult], Dict[int, List[ExtractionResult]]], 
                   filepath: str):
        """å¯¼å‡ºä¸ºæ–‡æœ¬æ ¼å¼"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("ä¿¡æ¯æŠ½å–ç»“æœæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            if isinstance(results, dict):
                for doc_id, doc_results in results.items():
                    f.write(f"æ–‡æ¡£ {doc_id}:\n")
                    f.write("-" * 30 + "\n")
                    
                    if doc_results:
                        # æŒ‰ç±»å‹åˆ†ç»„
                        by_type = defaultdict(list)
                        for result in doc_results:
                            by_type[result.entity_type].append(result)
                        
                        for entity_type, type_results in by_type.items():
                            f.write(f"\n{entity_type.upper()}:\n")
                            for result in sorted(type_results, key=lambda x: x.confidence, reverse=True):
                                f.write(f"  - {result.entity_value} (ç½®ä¿¡åº¦: {result.confidence:.3f})\n")
                    else:
                        f.write("  (æ— æŠ½å–ç»“æœ)\n")
                    
                    f.write("\n")
            else:
                # å•ä¸ªç»“æœåˆ—è¡¨
                by_type = defaultdict(list)
                for result in results:
                    by_type[result.entity_type].append(result)
                
                for entity_type, type_results in by_type.items():
                    f.write(f"\n{entity_type.upper()}:\n")
                    f.write("-" * 20 + "\n")
                    for result in sorted(type_results, key=lambda x: x.confidence, reverse=True):
                        f.write(f"  - {result.entity_value} (ç½®ä¿¡åº¦: {result.confidence:.3f})\n")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_documents_processed': 0,
            'total_extractions': 0,
            'total_processing_time': 0.0,
            'extractions_by_type': defaultdict(int),
            'extractions_by_extractor': defaultdict(int),
            'average_confidence': 0.0,
            'documents_with_extractions': 0
        }
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.results_cache.clear()
        print("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")
    
    def get_supported_entity_types(self) -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒçš„å®ä½“ç±»å‹"""
        all_types = set()
        for extractor in self.extractors.values():
            all_types.update(extractor.get_supported_entity_types())
        return sorted(list(all_types))


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æŠ½å–ç®¡ç†å™¨æµ‹è¯• ===")
    
    # åˆ›å»ºç®¡ç†å™¨
    config = {
        'enable_regex_extractor': True,
        'regex_confidence_threshold': 0.6,
        'merge_duplicate_entities': True,
        'max_entities_per_type': 20
    }
    
    manager = ExtractionManager(config)
    
    if not manager.initialize():
        print("âŒ ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
        exit(1)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "President Biden announced $2.5 billion aid to Ukraine yesterday.",
        "NPR's David Folkenflik reported from Washington, D.C.",
        '"This is crucial for democracy," said Secretary Austin.',
        "The meeting took place at the White House on March 15, 2024.",
    ]
    
    print(f"\nğŸ“‹ æ”¯æŒçš„å®ä½“ç±»å‹: {manager.get_supported_entity_types()}")
    
    # æµ‹è¯•å•ä¸ªæ–‡æœ¬æŠ½å–
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ æµ‹è¯•æ–‡æœ¬ {i}: {text}")
        results = manager.extract_from_text(text, doc_id=i, field="test")
        
        if results:
            for result in results:
                print(f"  âœ“ {result.entity_type}: '{result.entity_value}' (ç½®ä¿¡åº¦: {result.confidence:.3f})")
        else:
            print("  (æ— æŠ½å–ç»“æœ)")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æŠ½å–ç»Ÿè®¡:")
    stats = manager.get_summary_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    print("\nâœ… æŠ½å–ç®¡ç†å™¨æµ‹è¯•å®Œæˆï¼")