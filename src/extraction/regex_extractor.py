"""
åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„ä¿¡æ¯æŠ½å–å™¨
å®ç°äººåã€åœ°åã€ç»„ç»‡ã€æ—¶é—´ã€é‡‘é¢ç­‰ä¿¡æ¯çš„æŠ½å–
é’ˆå¯¹NPRæ–°é—»æ•°æ®ä¼˜åŒ–
"""

import re
import time
from typing import List, Dict, Any, Pattern, Set, Tuple
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.extraction.extractor_base import BaseExtractor, ExtractionResult
from src.utils.patterns import RegexPatterns

class RegexExtractor(BaseExtractor):
    """åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„ä¿¡æ¯æŠ½å–å™¨"""
    
    def __init__(self, confidence_threshold: float = 0.6):
        super().__init__("RegexExtractor")
        self.confidence_threshold = confidence_threshold
        self.patterns = {}
        self.pattern_descriptions = {}
        
        # ç½®ä¿¡åº¦æƒé‡é…ç½®ï¼ˆé’ˆå¯¹NPRæ•°æ®ä¼˜åŒ–ï¼‰
        self.confidence_weights = {
            'person': {
                'with_title': 0.95,     # President Biden, Dr. Smith
                'quoted_speech': 0.9,   # Biden said, according to Smith
                'npr_attribution': 0.9, # NPR's John Doe
                'standard_format': 0.8, # John Smith
                'middle_initial': 0.85, # John D. Smith
                'default': 0.7
            },
            'location': {
                'political_center': 0.95, # White House, Capitol Hill
                'country': 0.9,          # United States, China
                'state': 0.85,           # California, Texas
                'city_state': 0.9,       # Los Angeles, California
                'major_city': 0.8,       # New York, Chicago
                'international': 0.85,   # Ukraine, Gaza
                'default': 0.7
            },
            'organization': {
                'government': 0.95,      # Department of Defense, FBI
                'media': 0.9,           # NPR, CNN, New York Times
                'university': 0.85,      # Harvard University
                'corporation': 0.8,      # Microsoft Corp
                'international': 0.9,    # United Nations, NATO
                'default': 0.75
            },
            'time': {
                'full_date': 0.95,      # March 15, 2024
                'relative_time': 0.85,  # yesterday, last week
                'time_point': 0.8,      # 2:30 PM EST
                'political_time': 0.9,  # during the campaign
                'default': 0.7
            },
            'money': {
                'large_amount': 0.95,   # $2.5 billion
                'government_budget': 0.9, # budget of $100 million
                'currency_symbol': 0.85, # $1,000
                'percentage': 0.8,      # 8-10%
                'default': 0.75
            },
            'contact': {
                'official_email': 0.95, # @whitehouse.gov, @npr.org
                'government_phone': 0.9, # official numbers
                'url': 0.85,           # official websites
                'regular_email': 0.8,   # regular email addresses
                'default': 0.75
            },
            'quote': {
                'direct_quote': 0.9,    # "exact quote"
                'political_statement': 0.95, # official statements
                'expert_opinion': 0.85, # academic/expert quotes
                'indirect_quote': 0.75, # according to sources
                'default': 0.8
            }
        }
        
        # æ— æ•ˆåŒ¹é…è¿‡æ»¤å™¨
        self.invalid_patterns = {
            'person': [
                r'^\d+$',  # çº¯æ•°å­—
                r'^[A-Z]$',  # å•ä¸ªå¤§å†™å­—æ¯
                r'^(?:the|and|or|in|on|at|to|for|of|with|by|from|said|according|told)$',  # å¸¸è§è¯
                r'^(?:President|Senator|Representative|Governor|Dr|Mr|Mrs|Ms)$',  # åªæœ‰ç§°è°“
                r'^(?:NPR|CNN|BBC)$',  # åª’ä½“åç§°ï¼ˆåº”è¯¥æ˜¯ç»„ç»‡ï¼‰
            ],
            'location': [
                r'^\d+$',
                r'^[A-Z]$',
                r'^(?:the|and|or|in|on|at)$',
            ],
            'organization': [
                r'^\d+$',
                r'^[A-Z]$',
                r'^(?:the|and|or)$',
            ],
            'time': [
                r'^[A-Z]$',
                r'^(?:the|and|or)$',
            ],
            'money': [
                r'^[\$,\.]$',
                r'^(?:the|and|or)$',
            ],
            'contact': [
                r'^[@\.]$',
                r'^(?:www|http|https)$',
            ],
            'quote': [
                r'^["\']$',
                r'^[\.\,\!\?]$',
                r'^(?:said|the|and|or)$',
            ]
        }
        
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æŠ½å–å™¨"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨...")
            
            # è·å–ç¼–è¯‘åçš„æ¨¡å¼
            self.patterns = RegexPatterns.get_compiled_patterns()
            self.pattern_descriptions = RegexPatterns.get_pattern_descriptions()
            
            print(f"âœ… åŠ è½½äº† {len(self.patterns)} ç§å®ä½“ç±»å‹çš„æŠ½å–æ¨¡å¼")
            for entity_type, description in self.pattern_descriptions.items():
                pattern_count = len(self.patterns[entity_type])
                print(f"  ğŸ“‹ {entity_type}: {description} ({pattern_count} ä¸ªæ¨¡å¼)")
            
            print(f"âš™ï¸ ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æ­£åˆ™æŠ½å–å™¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def extract_from_text(self, text: str, doc_id: int, field: str) -> List[ExtractionResult]:
        """ä»æ–‡æœ¬ä¸­æŠ½å–ä¿¡æ¯"""
        if not self.is_initialized:
            print("âŒ é”™è¯¯ï¼šæŠ½å–å™¨æœªåˆå§‹åŒ–")
            return []
        
        if not text or not text.strip():
            return []
        
        start_time = time.time()
        results = []
        
        # å¯¹æ¯ç§å®ä½“ç±»å‹è¿›è¡ŒæŠ½å–
        for entity_type, pattern_list in self.patterns.items():
            entity_results = self._extract_entity_type(
                text, entity_type, pattern_list, doc_id, field
            )
            results.extend(entity_results)
        
        # åå¤„ç†ï¼šå»é‡ã€æ’åºã€è¿‡æ»¤
        results = self._post_process_results(results, text)
        
        # æ›´æ–°å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_time = time.time() - start_time
        self.statistics['processing_time'] += processing_time
        
        return results
    
    def _extract_entity_type(self, text: str, entity_type: str, 
                       pattern_list: List[Pattern], doc_id: int, 
                       field: str) -> List[ExtractionResult]:
        """æŠ½å–ç‰¹å®šç±»å‹çš„å®ä½“"""
        results = []
        matched_spans = set()  # é¿å…é‡å¤åŒ¹é…åŒä¸€ä½ç½®
        
        for pattern_idx, pattern in enumerate(pattern_list):
            try:
                for match in pattern.finditer(text):
                    # å¤„ç†åˆ†ç»„åŒ¹é… - ä¿®å¤ç‰ˆ
                    if match.groups():
                        # å¦‚æœæœ‰åˆ†ç»„ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºåˆ†ç»„
                        matched_text = None
                        start_pos = None
                        end_pos = None
                        
                        for i, group in enumerate(match.groups(), 1):
                            if group:  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºåˆ†ç»„
                                matched_text = group.strip()
                                start_pos = match.start(i)
                                end_pos = match.end(i)
                                break
                        
                        if not matched_text:  # å¦‚æœæ‰€æœ‰åˆ†ç»„éƒ½ä¸ºç©ºï¼Œä½¿ç”¨æ•´ä¸ªåŒ¹é…
                            matched_text = match.group().strip()
                            start_pos = match.start()
                            end_pos = match.end()
                    else:
                        # æ²¡æœ‰åˆ†ç»„ï¼Œä½¿ç”¨æ•´ä¸ªåŒ¹é…
                        matched_text = match.group().strip()
                        start_pos = match.start()
                        end_pos = match.end()
                    
                    # æ£€æŸ¥åŸºæœ¬æœ‰æ•ˆæ€§
                    if not matched_text or len(matched_text) < 2:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰åŒ¹é…é‡å 
                    span = (start_pos, end_pos)
                    if self._has_overlap(span, matched_spans):
                        continue
                    
                    # è¿‡æ»¤æ— æ•ˆåŒ¹é…
                    if not self._is_valid_match(matched_text, entity_type):
                        continue
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence = self._calculate_confidence(
                        matched_text, entity_type, pattern_idx, text, start_pos
                    )
                    
                    # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„ç»“æœ
                    if confidence >= self.confidence_threshold:
                        # è·å–ä¸Šä¸‹æ–‡
                        context = self._extract_context(text, start_pos, end_pos)
                        
                        # åˆ›å»ºæŠ½å–ç»“æœ
                        result = ExtractionResult(
                            entity_type=entity_type,
                            entity_value=matched_text,
                            confidence=confidence,
                            start_position=start_pos,
                            end_position=end_pos,
                            context=context,
                            doc_id=doc_id,
                            field=field,
                            metadata={
                                'pattern_index': pattern_idx,
                                'pattern_description': self._get_pattern_description(entity_type, pattern_idx),
                                'match_type': self._classify_match_type(matched_text, entity_type),
                                'has_groups': bool(match.groups())
                            }
                        )
                        
                        results.append(result)
                        matched_spans.add(span)
                        
            except Exception as e:
                print(f"âš ï¸ æ¨¡å¼åŒ¹é…å‡ºé”™ {entity_type}[{pattern_idx}]: {e}")
                continue
        
        return results


    def _calculate_confidence(self, matched_text: str, entity_type: str, 
                            pattern_idx: int, full_text: str, position: int) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        # è·å–åŒ¹é…ç±»å‹
        match_type = self._classify_match_type(matched_text, entity_type)
        
        # æ ¹æ®åŒ¹é…ç±»å‹è·å–åŸºç¡€ç½®ä¿¡åº¦
        if match_type in self.confidence_weights[entity_type]:
            base_confidence = self.confidence_weights[entity_type][match_type]
        else:
            base_confidence = self.confidence_weights[entity_type]['default']
        
        # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´ç½®ä¿¡åº¦
        context_boost = self._get_context_boost(matched_text, entity_type, full_text, position)
        
        # æ ¹æ®é•¿åº¦è°ƒæ•´ç½®ä¿¡åº¦
        length_boost = self._get_length_boost(matched_text, entity_type)
        
        # æœ€ç»ˆç½®ä¿¡åº¦
        final_confidence = min(1.0, base_confidence + context_boost + length_boost)
        
        return final_confidence
    
    def _classify_match_type(self, matched_text: str, entity_type: str) -> str:
        """åˆ†ç±»åŒ¹é…ç±»å‹ä»¥ç¡®å®šç½®ä¿¡åº¦æƒé‡ - æ”¹è¿›ç‰ˆ"""
        if entity_type == 'person':
            # æ£€æŸ¥æ˜¯å¦æœ‰èŒä½ç§°è°“
            if re.search(r'\b(?:President|Vice President|Senator|Representative|Governor|Dr|Prof|Secretary|Director|Chief|Justice)\b', matched_text, re.IGNORECASE):
                return 'with_title'
            
            # æ£€æŸ¥NPRå½’å±
            if 'NPR' in matched_text:
                return 'npr_attribution'
            
            # æ£€æŸ¥ä¸­é—´åæ ¼å¼
            if re.search(r'\b[A-Z][a-z]+\s+[A-Z]\.\s+[A-Z][a-z]+\b', matched_text):
                return 'middle_initial'
            
            # æ ‡å‡†ä¸¤ä¸ªè¯çš„äººå
            words = matched_text.strip().split()
            if len(words) == 2 and all(word[0].isupper() and word[1:].islower() for word in words):
                return 'standard_format'
                
        elif entity_type == 'location':
            # æ”¿æ²»ä¸­å¿ƒ
            political_centers = ['White House', 'Capitol Hill', 'Pentagon', 'Washington']
            if any(center.lower() in matched_text.lower() for center in political_centers):
                return 'political_center'
            
            # å›½é™…çƒ­ç‚¹
            international_hotspots = ['Ukraine', 'Russia', 'China', 'Iran', 'Israel', 'Gaza', 'Syria', 'Afghanistan', 'Iraq']
            if matched_text in international_hotspots:
                return 'international'
            
            # åŸå¸‚,å·æ ¼å¼
            if ',' in matched_text:
                return 'city_state'
            
            # å›½å®¶å
            countries = ['United States', 'America', 'Canada', 'Mexico', 'United Kingdom', 'Britain', 'France', 'Germany', 'Italy', 'Spain', 'Japan', 'Australia']
            if matched_text in countries:
                return 'country'
        
        elif entity_type == 'organization':
            # æ”¿åºœæœºæ„
            if re.search(r'\b(?:Department|FBI|CIA|NASA|Congress|Senate|White House|Supreme Court)\b', matched_text, re.IGNORECASE):
                return 'government'
            
            # åª’ä½“æœºæ„
            if re.search(r'\b(?:NPR|CNN|BBC|New York Times|Washington Post|CBS|NBC|ABC)\b', matched_text, re.IGNORECASE):
                return 'media'
            
            # å¤§å­¦
            if re.search(r'\b(?:University|College)\b', matched_text, re.IGNORECASE):
                return 'university'
            
            # å›½é™…ç»„ç»‡
            if re.search(r'\b(?:United Nations|NATO|EU|WHO|IMF)\b', matched_text, re.IGNORECASE):
                return 'international'
        
        elif entity_type == 'money':
            # å¤§é¢èµ„é‡‘
            if re.search(r'\b(?:billion|trillion)\b', matched_text, re.IGNORECASE):
                return 'large_amount'
            
            # ç™¾åˆ†æ¯”
            if '%' in matched_text:
                return 'percentage'
            
            # è´§å¸ç¬¦å·
            if matched_text.startswith('$'):
                return 'currency_symbol'
        
        elif entity_type == 'time':
            # å®Œæ•´æ—¥æœŸ
            if re.search(r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d+,?\s+\d{4}\b', matched_text):
                return 'full_date'
            
            # æ”¿æ²»æ—¶é—´
            if re.search(r'\b(?:campaign|election|inauguration|presidency|administration)\b', matched_text, re.IGNORECASE):
                return 'political_time'
            
            # ç›¸å¯¹æ—¶é—´
            if re.search(r'\b(?:today|yesterday|tomorrow|this week|last week)\b', matched_text, re.IGNORECASE):
                return 'relative_time'
        
        elif entity_type == 'contact':
            # å®˜æ–¹é‚®ç®±
            if re.search(r'@(?:whitehouse\.gov|npr\.org|.*\.gov)$', matched_text, re.IGNORECASE):
                return 'official_email'
            
            # URL
            if matched_text.startswith(('http', 'www')):
                return 'url'
            
            # ç”µè¯
            if re.search(r'\d{3}[-.\s]\d{3}[-.\s]\d{4}', matched_text):
                return 'phone'
        
        elif entity_type == 'quote':
            # é•¿å¼•ç”¨ï¼ˆå¯èƒ½æ˜¯é‡è¦å£°æ˜ï¼‰
            if len(matched_text) > 100:
                return 'political_statement'
            
            # ç›´æ¥å¼•ç”¨
            if matched_text.startswith('"') and matched_text.endswith('"'):
                return 'direct_quote'
        
        return 'default'

    
    def _get_context_boost(self, matched_text: str, entity_type: str, 
                      full_text: str, position: int) -> float:
        """æ ¹æ®ä¸Šä¸‹æ–‡è·å–ç½®ä¿¡åº¦æå‡"""
        context_size = 100
        start = max(0, position - context_size)
        end = min(len(full_text), position + len(matched_text) + context_size)
        context = full_text[start:end].lower()
        
        boost = 0.0
        
        if entity_type == 'person':
            # å¼ºçƒˆçš„äººåä¸Šä¸‹æ–‡æŒ‡æ ‡ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
            strong_person_indicators = [
                r'\b(?:president|senator|representative|governor|dr|prof|secretary|director|chief)\s*$',  # èŒä½åœ¨å‰
                r'^\s*(?:said|told|announced|declared|testified|stated|explained|noted)',  # å‘è¨€åŠ¨è¯åœ¨å
                r'\bnpr\'s\s*$',  # NPRå½’å±
                r'\b(?:according to|as)\s*$',  # å¼•ç”¨ä»‹ç»
            ]
            
            for indicator in strong_person_indicators:
                before_match = full_text[max(0, position-50):position].lower()
                after_match = full_text[position+len(matched_text):position+len(matched_text)+50].lower()
                
                if re.search(indicator, before_match) or re.search(r'^\s*(?:said|told|announced)', after_match):
                    boost += 0.15
                    break
            
            # å‡åˆ†æŒ‡æ ‡ - ä¸å¤ªå¯èƒ½æ˜¯äººåçš„ä¸Šä¸‹æ–‡
            negative_indicators = [
                r'\b(?:this|that|these|those|it|its|he|she|his|her|him|they|them|their)\s*$',  # ä»£è¯åœ¨å‰
                r'^\s*(?:is|was|are|were|will|would|can|could|should|may|might)',  # åŠ¨è¯åœ¨å
            ]
            
            for indicator in negative_indicators:
                before_match = full_text[max(0, position-30):position].lower()
                after_match = full_text[position+len(matched_text):position+len(matched_text)+30].lower()
                
                if re.search(indicator, before_match) or re.search(indicator, after_match):
                    boost -= 0.2  # å¤§å¹…å‡åˆ†
                    break
                    
        elif entity_type == 'organization':
            # å®˜æ–¹ä¸Šä¸‹æ–‡
            if re.search(r'\b(?:official|government|federal|administration|agency|bureau|department)\b', context):
                boost += 0.1
                
        elif entity_type == 'money':
            # æ”¿åºœ/é¢„ç®—ä¸Šä¸‹æ–‡
            if re.search(r'\b(?:budget|funding|appropriation|spending|allocation|investment|aid|grant)\b', context):
                boost += 0.15
            
            # ç»æµ/é‡‘èä¸Šä¸‹æ–‡
            if re.search(r'\b(?:revenue|profit|loss|cost|price|worth|value|tax|fee)\b', context):
                boost += 0.1
                
        elif entity_type == 'quote':
            # é‡è¦å‘è¨€äººä¸Šä¸‹æ–‡
            if re.search(r'\b(?:president|senator|secretary|director|spokesperson|official)\b', context):
                boost += 0.15
            
            # æ­£å¼åœºåˆä¸Šä¸‹æ–‡
            if re.search(r'\b(?:conference|hearing|testimony|statement|announcement|speech)\b', context):
                boost += 0.1
        
        return min(0.3, max(-0.3, boost))  # é™åˆ¶åœ¨-0.3åˆ°+0.3ä¹‹é—´

    def _get_length_boost(self, matched_text: str, entity_type: str) -> float:
        """æ ¹æ®é•¿åº¦è·å–ç½®ä¿¡åº¦è°ƒæ•´ - æ”¹è¿›ç‰ˆ"""
        length = len(matched_text.strip())
        
        if entity_type == 'person':
            if 5 <= length <= 25:  # åˆç†çš„äººåé•¿åº¦
                return 0.05
            elif length < 3:  # å¤ªçŸ­ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯å®Œæ•´äººå
                return -0.3
            elif length > 40:  # å¤ªé•¿ï¼Œå¯èƒ½åŒ…å«å…¶ä»–å†…å®¹
                return -0.2
                
        elif entity_type == 'quote':
            if 20 <= length <= 200:  # åˆç†çš„å¼•ç”¨é•¿åº¦
                return 0.1
            elif length < 10:  # å¤ªçŸ­çš„å¼•ç”¨ä¸å¯ä¿¡
                return -0.4
            elif length > 300:  # å¤ªé•¿å¯èƒ½åŒ…å«å¤šä¸ªå¥å­
                return -0.1
                
        elif entity_type == 'organization':
            if length < 3:  # ç»„ç»‡åå¤ªçŸ­
                return -0.3
            elif 3 <= length <= 50:  # åˆç†é•¿åº¦
                return 0.05
                
        elif entity_type == 'location':
            if length < 2:  # åœ°åå¤ªçŸ­
                return -0.3
            elif 2 <= length <= 30:  # åˆç†é•¿åº¦
                return 0.05
        
        return 0.0



    def _has_overlap(self, span: Tuple[int, int], existing_spans: Set[Tuple[int, int]]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰spané‡å """
        start, end = span
        for existing_start, existing_end in existing_spans:
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å 
            if not (end <= existing_start or start >= existing_end):
                # è®¡ç®—é‡å ç¨‹åº¦
                overlap_start = max(start, existing_start)
                overlap_end = min(end, existing_end)
                overlap_length = overlap_end - overlap_start
                
                # å¦‚æœé‡å è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                min_length = min(end - start, existing_end - existing_start)
                if overlap_length / min_length > 0.5:
                    return True
        return False
    
    """
    æ”¹è¿›çš„éªŒè¯é€»è¾‘
    ä¿®æ”¹ä½ç½®ï¼šsrc/extraction/regex_extractor.py ä¸­çš„ _is_valid_match æ–¹æ³•
    """

    def _is_valid_match(self, matched_text: str, entity_type: str) -> bool:
        """éªŒè¯åŒ¹é…æ˜¯å¦æœ‰æ•ˆ - æ”¹è¿›ç‰ˆ"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        text = matched_text.strip()
        if len(text) < 2:
            return False
        
        # é€šç”¨æ— æ•ˆè¯æ±‡
        common_invalid = {
            'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'is', 'was', 'are', 'were', 'will', 'would', 'can', 'could', 'should', 'may', 'might',
            'this', 'that', 'these', 'those', 'it', 'its', 'he', 'she', 'his', 'her', 'him',
            'they', 'them', 'their', 'we', 'us', 'our', 'i', 'me', 'my', 'you', 'your'
        }
        
        if text.lower() in common_invalid:
            return False
        
        # ç‰¹å®šç±»å‹çš„éªŒè¯
        if entity_type == 'person':
            return self._validate_person_name(text)
        elif entity_type == 'location':
            return self._validate_location(text)
        elif entity_type == 'organization':
            return self._validate_organization(text)
        elif entity_type == 'money':
            return self._validate_money(text)
        elif entity_type == 'contact':
            return self._validate_contact(text)
        elif entity_type == 'quote':
            return self._validate_quote(text)
        elif entity_type == 'time':
            return self._validate_time(text)
        
        return True

    def _validate_person_name(self, text: str) -> bool:
        """éªŒè¯äººåçš„æœ‰æ•ˆæ€§"""
        # æ— æ•ˆçš„äººåæ¨¡å¼
        invalid_person_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[A-Z]$',  # å•ä¸ªå­—æ¯
            r'^(?:said|told|announced|declared|stated|explained|noted|added|continued)$',  # åŠ¨è¯
            r'^(?:support|democracy|crucial|important|necessary|significant)$',  # å½¢å®¹è¯/åè¯
            r'^(?:this|that|these|those|it|its)(?:\s+\w+)*$',  # ä»£è¯å¼€å¤´
            r'^(?:the|and|or|in|on|at|to|for|of|with|by|from)(?:\s+\w+)*$',  # ä»‹è¯/è¿è¯å¼€å¤´
            r'^\w+(?:\s+said|\s+told|\s+announced)$',  # ä»¥åŠ¨è¯ç»“å°¾
        ]
        
        for pattern in invalid_person_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return False
        
        # äººååº”è¯¥ä»¥å¤§å†™å­—æ¯å¼€å¤´
        if not text[0].isupper():
            return False
        
        # äººåé•¿åº¦åˆç†æ€§æ£€æŸ¥
        if len(text) > 50:  # å¤ªé•¿
            return False
        
        # äººåä¸åº”è¯¥åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆé™¤äº†ç‚¹å’Œæ’‡å·ï¼‰
        if re.search(r'[^\w\s\.\'-]', text):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„éäººåè¯æ±‡
        non_name_words = {
            'support', 'democracy', 'crucial', 'important', 'necessary', 'significant',
            'government', 'administration', 'policy', 'program', 'system', 'process',
            'development', 'research', 'study', 'report', 'analysis', 'data',
            'information', 'details', 'facts', 'evidence', 'proof', 'confirmation'
        }
        
        words = text.lower().split()
        if any(word in non_name_words for word in words):
            return False
        
        return True

    def _validate_location(self, text: str) -> bool:
        """éªŒè¯åœ°åçš„æœ‰æ•ˆæ€§"""
        # åœ°ååº”è¯¥ä»¥å¤§å†™å­—æ¯å¼€å¤´
        if not text[0].isupper():
            return False
        
        # æ— æ•ˆçš„åœ°åæ¨¡å¼
        invalid_location_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[A-Z]$',  # å•ä¸ªå­—æ¯
            r'^(?:said|told|announced|declared|stated)$',  # åŠ¨è¯
            r'^(?:the|and|or|in|on|at)(?:\s+\w+)*$',  # ä»‹è¯å¼€å¤´
        ]
        
        for pattern in invalid_location_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return False
        
        return True

    def _validate_organization(self, text: str) -> bool:
        """éªŒè¯ç»„ç»‡æœºæ„çš„æœ‰æ•ˆæ€§"""
        # ç»„ç»‡ååº”è¯¥ä»¥å¤§å†™å­—æ¯å¼€å¤´
        if not text[0].isupper():
            return False
        
        # ç»„ç»‡åä¸åº”è¯¥å¤ªçŸ­
        if len(text) < 3:
            return False
        
        return True

    def _validate_money(self, text: str) -> bool:
        """éªŒè¯é‡‘é¢çš„æœ‰æ•ˆæ€§"""
        # é‡‘é¢åº”è¯¥åŒ…å«æ•°å­—
        if not re.search(r'\d', text):
            return False
        
        # æ£€æŸ¥æ— æ•ˆçš„é‡‘é¢æ ¼å¼
        if re.match(r'^[\$,\.%]+$', text):  # åªæœ‰ç¬¦å·
            return False
        
        return True

    def _validate_contact(self, text: str) -> bool:
        """éªŒè¯è”ç³»æ–¹å¼çš„æœ‰æ•ˆæ€§"""
        # é‚®ç®±æ ¼å¼éªŒè¯
        if '@' in text:
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, text))
        
        # ç”µè¯å·ç éªŒè¯
        if re.search(r'\d{3}', text):
            return True
        
        # URLéªŒè¯
        if text.startswith(('http', 'www', 'ftp')):
            return True
        
        return True

    def _validate_quote(self, text: str) -> bool:
        """éªŒè¯å¼•ç”¨å†…å®¹çš„æœ‰æ•ˆæ€§"""
        # å¼•ç”¨ä¸åº”è¯¥å¤ªçŸ­
        if len(text) < 10:
            return False
        
        # å¼•ç”¨ä¸åº”è¯¥åªæ˜¯æ ‡ç‚¹ç¬¦å·
        if re.match(r'^[\.\,\!\?\;\:\"\']+$', text):
            return False
        
        # å¼•ç”¨åº”è¯¥åŒ…å«å®é™…å†…å®¹ï¼ˆå­—æ¯ï¼‰
        if not re.search(r'[a-zA-Z]', text):
            return False
        
        return True

    def _validate_time(self, text: str) -> bool:
        """éªŒè¯æ—¶é—´ä¿¡æ¯çš„æœ‰æ•ˆæ€§"""
        # æ—¶é—´è¡¨è¾¾ä¸åº”è¯¥å¤ªçŸ­
        if len(text) < 3:
            return False
        
        return True
    
    def _extract_context(self, text: str, start: int, end: int, context_size: int = 60) -> str:
        """æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_start = max(0, start - context_size)
        context_end = min(len(text), end + context_size)
        
        context = text[context_start:context_end]
        
        # åœ¨åŒ¹é…çš„å®ä½“å‘¨å›´åŠ æ ‡è®°
        entity_in_context = text[start:end]
        relative_start = start - context_start
        relative_end = end - context_start
        
        marked_context = (
            context[:relative_start] + 
            f"[{entity_in_context}]" + 
            context[relative_end:]
        )
        
        # æ¸…ç†ä¸Šä¸‹æ–‡ï¼ˆç§»é™¤å¤šä½™çš„ç©ºç™½ï¼‰
        marked_context = re.sub(r'\s+', ' ', marked_context.strip())
        
        return marked_context
    
    def _get_pattern_description(self, entity_type: str, pattern_idx: int) -> str:
        """è·å–æ¨¡å¼æè¿°"""
        descriptions = {
            'person': [
                'æ”¿æ²»äººç‰©å’Œå®˜å‘˜', 'NPRè®°è€…å½’å±', 'å¼•ç”¨ä¸­çš„äººå', 
                'æ ‡å‡†äººåæ ¼å¼', 'å¸¦ä¸­é—´å', 'å…¶ä»–äººåæ¨¡å¼', 'è¡¥å……æ¨¡å¼'
            ],
            'location': [
                'æ”¿æ²»ä¸­å¿ƒ', 'å›½é™…çƒ­ç‚¹', 'ç¾å›½å·å', 
                'åŸå¸‚å·æ ¼å¼', 'ä¸»è¦åŸå¸‚', 'å…¶ä»–åœ°å', 'è¡¥å……æ¨¡å¼'
            ],
            'organization': [
                'æ”¿åºœæœºæ„', 'åª’ä½“ç»„ç»‡', 'å¤§å­¦é™¢æ ¡', 
                'ä¼ä¸šå…¬å¸', 'å›½é™…ç»„ç»‡', 'å…¶ä»–ç»„ç»‡'
            ],
            'time': [
                'æ–°é—»æ—¶é—´', 'å…·ä½“æ—¥æœŸ', 'ç›¸å¯¹æ—¶é—´', 
                'æ—¶é—´æ®µ', 'æ”¿æ²»æ—¶é—´', 'æ—¶é—´ç‚¹', 'å…¶ä»–æ—¶é—´', 'è¡¥å……æ¨¡å¼'
            ],
            'money': [
                'å¤§é¢èµ„é‡‘', 'æ”¿åºœé¢„ç®—', 'è´§å¸ç¬¦å·', 
                'ç™¾åˆ†æ¯”', 'å…¶ä»–é‡‘é¢', 'è¡¥å……æ¨¡å¼'
            ],
            'contact': [
                'å®˜æ–¹é‚®ç®±', 'æ”¿åºœç”µè¯', 'å®˜æ–¹ç½‘å€', 
                'æ™®é€šé‚®ç®±', 'å…¶ä»–è”ç³»æ–¹å¼', 'è¡¥å……æ¨¡å¼'
            ],
            'quote': [
                'æ”¿æ²»å£°æ˜', 'ç›´æ¥å¼•ç”¨', 'ä¸“å®¶è§‚ç‚¹', 'é—´æ¥å¼•ç”¨'
            ]
        }
        
        if entity_type in descriptions and pattern_idx < len(descriptions[entity_type]):
            return descriptions[entity_type][pattern_idx]
        
        return f"{entity_type}æ¨¡å¼{pattern_idx}"
    
    def _post_process_results(self, results: List[ExtractionResult], text: str) -> List[ExtractionResult]:
        """åå¤„ç†æŠ½å–ç»“æœ"""
        if not results:
            return results
        
        # 1. æŒ‰ä½ç½®æ’åº
        results.sort(key=lambda x: x.start_position)
        
        # 2. å»é‡ç›¸ä¼¼ç»“æœ
        filtered_results = []
        for result in results:
            if not self._is_duplicate_result(result, filtered_results):
                filtered_results.append(result)
        
        # 3. æŒ‰ç½®ä¿¡åº¦è¿‡æ»¤å’Œæ’åº
        high_confidence_results = [
            r for r in filtered_results 
            if r.confidence >= self.confidence_threshold
        ]
        
        # 4. æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆåŒç±»å‹å†…éƒ¨ï¼‰
        final_results = sorted(high_confidence_results, 
                             key=lambda x: (x.entity_type, -x.confidence, x.start_position))
        
        return final_results
    
    def _is_duplicate_result(self, result: ExtractionResult, existing_results: List[ExtractionResult]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤ç»“æœ"""
        for existing in existing_results:
            # 1. å®Œå…¨ç›¸åŒçš„å®ä½“
            if (result.entity_type == existing.entity_type and 
                result.entity_value.lower() == existing.entity_value.lower()):
                return True
            
            # 2. é«˜åº¦é‡å çš„ä½ç½®
            overlap_start = max(result.start_position, existing.start_position)
            overlap_end = min(result.end_position, existing.end_position)
            
            if overlap_start < overlap_end:  # æœ‰é‡å 
                overlap_length = overlap_end - overlap_start
                min_length = min(
                    result.end_position - result.start_position,
                    existing.end_position - existing.start_position
                )
                
                if overlap_length / min_length > 0.8:  # 80%é‡å 
                    # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„
                    return result.confidence <= existing.confidence
        
        return False
    
    def get_supported_entity_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„å®ä½“ç±»å‹åˆ—è¡¨"""
        return list(self.patterns.keys()) if self.patterns else []
    
    def get_extraction_summary(self, results: List[ExtractionResult]) -> Dict[str, Any]:
        """è·å–æŠ½å–ç»“æœæ‘˜è¦"""
        if not results:
            return {
                'total_extractions': 0,
                'entity_type_counts': {},
                'avg_confidence': 0.0,
                'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0},
                'field_distribution': {},
                'top_entities': {}
            }
        
        summary = {
            'total_extractions': len(results),
            'entity_type_counts': {},
            'avg_confidence': 0.0,
            'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'field_distribution': {},
            'top_entities': {},
            'quality_metrics': {}
        }
        
        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        for result in results:
            entity_type = result.entity_type
            if entity_type not in summary['entity_type_counts']:
                summary['entity_type_counts'][entity_type] = 0
            summary['entity_type_counts'][entity_type] += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        total_confidence = sum(r.confidence for r in results)
        summary['avg_confidence'] = total_confidence / len(results)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        for result in results:
            if result.confidence >= 0.9:
                summary['confidence_distribution']['high'] += 1
            elif result.confidence >= 0.7:
                summary['confidence_distribution']['medium'] += 1
            else:
                summary['confidence_distribution']['low'] += 1
        
        # å­—æ®µåˆ†å¸ƒ
        for result in results:
            field = result.field
            if field not in summary['field_distribution']:
                summary['field_distribution'][field] = 0
            summary['field_distribution'][field] += 1
        
        # æ¯ç§ç±»å‹çš„çƒ­é—¨å®ä½“
        entity_counts = {}
        for result in results:
            key = f"{result.entity_type}:{result.entity_value}"
            if key not in entity_counts:
                entity_counts[key] = {'count': 0, 'avg_confidence': 0, 'confidences': []}
            entity_counts[key]['count'] += 1
            entity_counts[key]['confidences'].append(result.confidence)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦å¹¶è·å–çƒ­é—¨å®ä½“
        for entity_type in summary['entity_type_counts']:
            type_entities = []
            for entity_key, stats in entity_counts.items():
                if entity_key.startswith(entity_type + ':'):
                    entity_name = entity_key.split(':', 1)[1]
                    avg_conf = sum(stats['confidences']) / len(stats['confidences'])
                    type_entities.append((entity_name, stats['count'], avg_conf))
            
            # æŒ‰å‡ºç°æ¬¡æ•°å’Œç½®ä¿¡åº¦æ’åº
            type_entities.sort(key=lambda x: (x[1], x[2]), reverse=True)
            summary['top_entities'][entity_type] = type_entities[:5]  # å‰5ä¸ª
        
        # è´¨é‡æŒ‡æ ‡
        summary['quality_metrics'] = {
            'high_confidence_ratio': summary['confidence_distribution']['high'] / len(results),
            'avg_entities_per_type': len(results) / len(summary['entity_type_counts']) if summary['entity_type_counts'] else 0,
            'most_common_type': max(summary['entity_type_counts'], key=summary['entity_type_counts'].get) if summary['entity_type_counts'] else None
        }
        
        return summary


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨æµ‹è¯• ===")
    
    # ä½¿ç”¨NPRé£æ ¼çš„æµ‹è¯•æ–‡æœ¬
    test_text = """
    President Joe Biden announced today that the United States will provide
    $2.5 billion in aid to Ukraine. The announcement was made at the White House
    during a meeting with Ukrainian President Volodymyr Zelenskyy on March 15, 2024.
    
    "This support is crucial for democracy," Biden said during the press conference
    at 2:30 PM EST. NPR's David Folkenflik reported from Washington, D.C.
    
    The funding will be distributed through the Department of Defense according to
    Secretary Lloyd Austin. Katherine Maher, NPR's CEO, testified before Congress yesterday.
    
    The Corporation for Public Broadcasting typically receives about 8-10% of revenues
    from federal sources. For information, contact the White House at 202-456-1414 
    or email press@whitehouse.gov. Visit https://npr.org for updates.
    
    "We are committed to supporting our international allies," said the President
    during the ceremony in the Rose Garden.
    """
    
    # åˆå§‹åŒ–æŠ½å–å™¨
    print("åˆå§‹åŒ–æŠ½å–å™¨...")
    extractor = RegexExtractor(confidence_threshold=0.6)
    
    if not extractor.initialize():
        print("âŒ æŠ½å–å™¨åˆå§‹åŒ–å¤±è´¥")
        exit(1)
    
    print(f"\nğŸ“„ æµ‹è¯•æ–‡æœ¬ ({len(test_text)} å­—ç¬¦):")
    print("=" * 80)
    print(test_text)
    print("=" * 80)
    
    # æ‰§è¡ŒæŠ½å–
    print("\nğŸ” å¼€å§‹ä¿¡æ¯æŠ½å–...")
    results = extractor.extract_from_text(test_text, doc_id=1, field="content")
    
    print(f"\nâœ… æŠ½å–å®Œæˆï¼æ‰¾åˆ° {len(results)} ä¸ªå®ä½“")
    print("=" * 80)
    
    # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºç»“æœ
    results_by_type = {}
    for result in results:
        if result.entity_type not in results_by_type:
            results_by_type[result.entity_type] = []
        results_by_type[result.entity_type].append(result)
    
    for entity_type, type_results in results_by_type.items():
        print(f"\nğŸ·ï¸ ã€{entity_type.upper()}ã€‘({len(type_results)} ä¸ª)")
        for i, result in enumerate(sorted(type_results, key=lambda x: x.confidence, reverse=True), 1):
            print(f"  {i}. {result.entity_value}")
            print(f"     ç½®ä¿¡åº¦: {result.confidence:.3f} | {result.metadata.get('match_type', 'default')}")
            print(f"     ä½ç½®: {result.start_position}-{result.end_position}")
            print(f"     æ¨¡å¼: {result.metadata.get('pattern_description', 'unknown')}")
            print(f"     ä¸Šä¸‹æ–‡: ...{result.context[:80]}...")
            print()
    
    # æ˜¾ç¤ºæŠ½å–æ‘˜è¦
    print("=" * 80)
    print("ğŸ“Š æŠ½å–æ‘˜è¦:")
    summary = extractor.get_extraction_summary(results)
    
    print(f"  ğŸ“ˆ æ€»è®¡: {summary['total_extractions']} ä¸ªå®ä½“")
    print(f"  ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {summary['avg_confidence']:.3f}")
    print(f"  ğŸ“‹ ç±»å‹åˆ†å¸ƒ: {summary['entity_type_counts']}")
    print(f"  ğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ: {summary['confidence_distribution']}")
    print(f"  ğŸ“„ å­—æ®µåˆ†å¸ƒ: {summary['field_distribution']}")
    print(f"  â­ è´¨é‡æŒ‡æ ‡:")
    print(f"     é«˜ç½®ä¿¡åº¦å æ¯”: {summary['quality_metrics']['high_confidence_ratio']:.1%}")
    print(f"     å¹³å‡æ¯ç±»å®ä½“æ•°: {summary['quality_metrics']['avg_entities_per_type']:.1f}")
    print(f"     æœ€å¸¸è§ç±»å‹: {summary['quality_metrics']['most_common_type']}")
    
    # æ˜¾ç¤ºçƒ­é—¨å®ä½“
    print(f"\nğŸ”¥ çƒ­é—¨å®ä½“:")
    for entity_type, top_entities in summary['top_entities'].items():
        if top_entities:
            print(f"  {entity_type}: ", end="")
            top_3 = [f"{name}({count})" for name, count, conf in top_entities[:3]]
            print(", ".join(top_3))
    
    print("\nâœ… æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å™¨æµ‹è¯•å®Œæˆï¼")
    print("ğŸ¯ ä¸‹ä¸€æ­¥ï¼šå®ç°æŠ½å–ç®¡ç†å™¨")