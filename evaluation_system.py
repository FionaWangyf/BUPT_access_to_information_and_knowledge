#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPRæ–‡ç« ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ - è¯„ä»·ç³»ç»Ÿ
åŒ…å«äººå·¥è¯„ä»·å·¥å…·å’Œè‡ªåŠ¨è¯„ä»·æŒ‡æ ‡è®¡ç®—

ä½œè€…: [ä½ çš„å§“å]
æ—¥æœŸ: 2025-05-30
"""

import sys
import os
sys.path.append('src')

from src.retrieval.search_engine import EnhancedSearchEngine
from src.evaluation.manual_evaluation import ManualEvaluator
from src.evaluation.evaluation_metrics import EvaluationMetrics, EvaluationReport
from src.evaluation.test_queries import TestQueryManager
import argparse
import json

class EvaluationSystem:
    """å®Œæ•´çš„è¯„ä»·ç³»ç»Ÿï¼šæ•´åˆäººå·¥è¯„ä»·å’Œè‡ªåŠ¨æŒ‡æ ‡è®¡ç®—"""
    
    def __init__(self, data_file: str):
        self.data_file = data_file
        self.search_engine = None
        self.manual_evaluator = None
        self.metrics_calculator = EvaluationMetrics()
        self.report_generator = EvaluationReport()
        self.query_manager = TestQueryManager()
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯„ä»·ç³»ç»Ÿ"""
        print("ğŸ”§ åˆå§‹åŒ–è¯„ä»·ç³»ç»Ÿ...")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if not os.path.exists(self.data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            return False
        
        # åˆå§‹åŒ–æœç´¢å¼•æ“
        self.search_engine = EnhancedSearchEngine(self.data_file)
        if not self.search_engine.initialize():
            print("âŒ æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åŠ è½½æˆ–åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
        print("æ­£åœ¨åŠ è½½æµ‹è¯•æŸ¥è¯¢...")
        if not self.query_manager.load_queries():
            print("åˆ›å»ºé»˜è®¤æµ‹è¯•æŸ¥è¯¢é›†åˆ...")
            self.query_manager.create_default_queries()
            self.query_manager.save_queries()
        
        # æ˜¾ç¤ºåŠ è½½çš„æŸ¥è¯¢æ•°é‡
        queries = self.query_manager.get_all_queries()
        print(f"å·²åŠ è½½ {len(queries)} ä¸ªæµ‹è¯•æŸ¥è¯¢")
        for query in queries:
            print(f"æŸ¥è¯¢ {query.query_id}: {query.query_text}")
        
        # åˆå§‹åŒ–äººå·¥è¯„ä»·å™¨ï¼Œä¼ å…¥æŸ¥è¯¢ç®¡ç†å™¨
        self.manual_evaluator = ManualEvaluator(self.search_engine, self.query_manager)
        
        print("âœ… è¯„ä»·ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
        return True
    
    def run_manual_evaluation(self):
        """è¿è¡Œäººå·¥è¯„ä¼°"""
        print("\n=== äººå·¥è¯„ä¼°æ¨¡å¼ ===")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        
        while True:
            try:
                command = input("\nè¯·è¾“å…¥å‘½ä»¤: ").strip()
                
                if command == "help":
                    print("\nå¯ç”¨å‘½ä»¤:")
                    print("  list - æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•æŸ¥è¯¢")
                    print("  eval <query_id> - è¯„ä¼°æŒ‡å®šæŸ¥è¯¢")
                    print("  batch [query_ids] - æ‰¹é‡è¯„ä¼°å¤šä¸ªæŸ¥è¯¢")
                    print("  summary - æ˜¾ç¤ºè¯„ä¼°æ‘˜è¦")
                    print("  add <query_text> [description] - æ·»åŠ è‡ªå®šä¹‰æŸ¥è¯¢")
                    print("  quit - é€€å‡ºè¯„ä¼°")
                
                elif command == "list":
                    self.query_manager.display_queries()
                
                elif command.startswith("eval "):
                    try:
                        query_id = int(command.split()[1])
                        self.manual_evaluator.evaluate_query(query_id)
                    except (IndexError, ValueError):
                        print("é”™è¯¯: è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢ID")
                
                elif command.startswith("batch "):
                    try:
                        query_ids = [int(id) for id in command.split()[1:]]
                        self.manual_evaluator.batch_evaluate(query_ids)
                    except (IndexError, ValueError):
                        print("é”™è¯¯: è¯·æä¾›æœ‰æ•ˆçš„æŸ¥è¯¢IDåˆ—è¡¨")
                
                elif command == "summary":
                    self.manual_evaluator.show_summary()
                
                elif command.startswith("add "):
                    try:
                        # è§£æå‘½ä»¤å‚æ•°
                        parts = command[4:].strip()
                        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªç©ºæ ¼ä½œä¸ºæŸ¥è¯¢æ–‡æœ¬å’Œæè¿°çš„åˆ†éš”ç¬¦
                        first_space = parts.find(" ")
                        if first_space == -1:
                            # å¦‚æœæ²¡æœ‰ç©ºæ ¼ï¼Œæ•´ä¸ªæ–‡æœ¬éƒ½æ˜¯æŸ¥è¯¢
                            query_text = parts
                            description = ""
                        else:
                            # åˆ†å‰²æŸ¥è¯¢æ–‡æœ¬å’Œæè¿°
                            query_text = parts[:first_space]
                            description = parts[first_space:].strip()
                        
                        # æ·»åŠ æ–°æŸ¥è¯¢
                        query = self.query_manager.add_custom_query(query_text, description)
                        
                        # ç«‹å³è¯„ä¼°æ–°æ·»åŠ çš„æŸ¥è¯¢
                        print("\næ˜¯å¦ç«‹å³è¯„ä¼°è¿™ä¸ªæ–°æŸ¥è¯¢ï¼Ÿ(y/n)")
                        if input().lower() == 'y':
                            self.manual_evaluator.evaluate_query(query.query_id)
                            
                    except Exception as e:
                        print(f"é”™è¯¯: æ·»åŠ æŸ¥è¯¢å¤±è´¥ - {str(e)}")
                
                elif command == "quit":
                    break
                
                else:
                    print("æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
                    
            except KeyboardInterrupt:
                print("\nè¯„ä¼°å·²ä¸­æ–­")
                break
            except Exception as e:
                print(f"é”™è¯¯: {str(e)}")
    
    def calculate_system_metrics(self, evaluation_file: str = "results/manual_evaluation.json"):
        """è®¡ç®—ç³»ç»Ÿè¯„ä»·æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—ç³»ç»Ÿè¯„ä»·æŒ‡æ ‡...")
        
        if not os.path.exists(evaluation_file):
            print(f"âŒ è¯„ä»·æ–‡ä»¶ä¸å­˜åœ¨: {evaluation_file}")
            print("è¯·å…ˆè¿›è¡Œäººå·¥è¯„ä»·")
            return
        
        try:
            # åŠ è½½è¯„ä»·æ•°æ®
            with open(evaluation_file, 'r', encoding='utf-8') as f:
                evaluations = json.load(f)
            
            if not evaluations:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä»·æ•°æ®")
                return
            
            # å‡†å¤‡æŸ¥è¯¢æ•°æ®
            queries_data = []
            for query_id, eval_data in evaluations.items():
                # æå–ç›¸å…³æ–‡æ¡£å’Œæ£€ç´¢ç»“æœ
                relevant_docs = []
                retrieved_docs = []
                relevance_scores = []
                
                for result in eval_data.get('results_evaluated', []):
                    doc_id = result['doc_id']
                    relevance = result['relevance_score']
                    
                    retrieved_docs.append(doc_id)
                    relevance_scores.append(relevance)
                    
                    if relevance > 0:  # ç›¸å…³æ–‡æ¡£ï¼ˆè¯„åˆ†>0ï¼‰
                        relevant_docs.append(doc_id)
                
                if retrieved_docs:  # åªåŒ…å«æœ‰ç»“æœçš„æŸ¥è¯¢
                    queries_data.append({
                        'query_id': int(query_id),
                        'query_text': eval_data['query_text'],
                        'relevant_docs': relevant_docs,
                        'retrieved_docs': retrieved_docs,
                        'relevance_scores': relevance_scores
                    })
            
            if not queries_data:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä»·æ•°æ®")
                return
            
            print(f"ğŸ“ˆ åˆ†æ {len(queries_data)} ä¸ªæŸ¥è¯¢çš„è¯„ä»·ç»“æœ...")
            
            # ç”Ÿæˆç³»ç»ŸæŠ¥å‘Š
            system_report = self.report_generator.generate_system_report(queries_data)
            
            # æ˜¾ç¤ºæŠ¥å‘Š
            formatted_report = self.report_generator.format_report(system_report, "system")
            print("\n" + formatted_report)
            
            # ç”Ÿæˆè¯¦ç»†çš„æ¯æŸ¥è¯¢æŠ¥å‘Š
            print(f"\n{'='*80}")
            print(f"è¯¦ç»†æŸ¥è¯¢åˆ†æ")
            print(f"{'='*80}")
            
            for qd in queries_data:
                query_report = self.report_generator.generate_query_report(
                    qd['query_id'], qd['query_text'], 
                    qd['relevant_docs'], qd['retrieved_docs'],
                    qd['relevance_scores']
                )
                
                print(f"\næŸ¥è¯¢ {qd['query_id']}: '{qd['query_text']}'")
                print(f"ç›¸å…³æ–‡æ¡£: {len(qd['relevant_docs'])}, æ£€ç´¢æ–‡æ¡£: {len(qd['retrieved_docs'])}")
                
                metrics = query_report['metrics']
                key_metrics = ['Precision@5', 'Recall@5', 'F1@5', 'NDCG@5', 'Average_Precision']
                for metric in key_metrics:
                    if metric in metrics:
                        print(f"  {metric}: {metrics[metric]:.3f}")
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            report_file = "results/evaluation_report.json"
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            
            full_report = {
                'system_report': system_report,
                'query_reports': [
                    self.report_generator.generate_query_report(
                        qd['query_id'], qd['query_text'], 
                        qd['relevant_docs'], qd['retrieved_docs'],
                        qd['relevance_scores']
                    ) for qd in queries_data
                ]
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(full_report, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            print(f"âŒ è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def demo_evaluation(self):
        """æ¼”ç¤ºè¯„ä»·åŠŸèƒ½"""
        print("ğŸ¯ æ¼”ç¤ºè¯„ä»·åŠŸèƒ½...")
        
        # åˆ›å»ºä¸€äº›ç¤ºä¾‹è¯„ä»·æ•°æ®
        demo_queries = [
            {
                'query_id': 1,
                'query_text': 'climate change',
                'relevant_docs': [0, 2, 5],
                'retrieved_docs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                'relevance_scores': [3, 0, 2, 0, 0, 1, 0, 0, 0, 0]
            },
            {
                'query_id': 2,
                'query_text': 'health care',
                'relevant_docs': [1, 3, 7],
                'retrieved_docs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                'relevance_scores': [0, 2, 0, 3, 0, 0, 0, 1, 0, 0]
            }
        ]
        
        print(f"ğŸ“Š ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®—è¯„ä»·æŒ‡æ ‡...")
        
        for qd in demo_queries:
            print(f"\næŸ¥è¯¢: '{qd['query_text']}'")
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self.metrics_calculator.calculate_all_metrics(
                qd['relevant_docs'], qd['retrieved_docs'], qd['relevance_scores']
            )
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            key_metrics = ['Precision@5', 'Recall@5', 'F1@5', 'NDCG@5', 'Average_Precision']
            for metric in key_metrics:
                if metric in metrics:
                    print(f"  {metric}: {metrics[metric]:.3f}")
        
        # è®¡ç®—ç³»ç»Ÿçº§æŒ‡æ ‡
        results = [(qd['relevant_docs'], qd['retrieved_docs']) for qd in demo_queries]
        map_score = self.metrics_calculator.mean_average_precision(results)
        mrr_score = self.metrics_calculator.mean_reciprocal_rank(results)
        
        print(f"\nç³»ç»Ÿçº§æŒ‡æ ‡:")
        print(f"  MAP (Mean Average Precision): {map_score:.3f}")
        print(f"  MRR (Mean Reciprocal Rank): {mrr_score:.3f}")
    
    def interactive_menu(self):
        """äº¤äº’å¼èœå•"""
        while True:
            print(f"\n{'='*60}")
            print(f"ğŸ” NPRæ£€ç´¢ç³»ç»Ÿè¯„ä»·å·¥å…·")
            print(f"{'='*60}")
            print(f"1. è¿è¡Œäººå·¥è¯„ä»·")
            print(f"2. è®¡ç®—ç³»ç»Ÿè¯„ä»·æŒ‡æ ‡")
            print(f"3. æ¼”ç¤ºè¯„ä»·åŠŸèƒ½")
            print(f"4. æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯")
            print(f"5. é€€å‡º")
            
            try:
                choice = input(f"\nè¯·é€‰æ‹©åŠŸèƒ½ (1-5): ").strip()
                
                if choice == '1':
                    self.run_manual_evaluation()
                
                elif choice == '2':
                    self.calculate_system_metrics()
                
                elif choice == '3':
                    self.demo_evaluation()
                
                elif choice == '4':
                    if self.search_engine:
                        info = self.search_engine.get_system_info()
                        print(f"\nğŸ“‹ ç³»ç»Ÿä¿¡æ¯:")
                        for key, value in info.items():
                            if isinstance(value, dict):
                                print(f"{key}:")
                                for k, v in value.items():
                                    print(f"  {k}: {v}")
                            else:
                                print(f"{key}: {value}")
                
                elif choice == '5':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-5")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="NPRæ£€ç´¢ç³»ç»Ÿè¯„ä»·å·¥å…·")
    parser.add_argument("--mode", choices=['manual', 'metrics', 'demo'], 
                       help="è¿è¡Œæ¨¡å¼: manual=äººå·¥è¯„ä»·, metrics=è®¡ç®—æŒ‡æ ‡, demo=æ¼”ç¤º")
    parser.add_argument("--eval-file", default="results/manual_evaluation.json",
                       help="è¯„ä»·ç»“æœæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "data/npr_articles.json"
    
    # åˆ›å»ºè¯„ä»·ç³»ç»Ÿ
    eval_system = EvaluationSystem(data_file)
    
    if not eval_system.initialize():
        print("âŒ è¯„ä»·ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return
    
    try:
        if args.mode == 'manual':
            eval_system.run_manual_evaluation()
        elif args.mode == 'metrics':
            eval_system.calculate_system_metrics(args.eval_file)
        elif args.mode == 'demo':
            eval_system.demo_evaluation()
        else:
            # é»˜è®¤äº¤äº’æ¨¡å¼
            eval_system.interactive_menu()
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()